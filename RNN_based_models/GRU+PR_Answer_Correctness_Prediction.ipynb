{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU+PR_Answer_Correctness_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "farifxiKU1aB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import random\n",
        "from random import choice\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, GRU, Concatenate, Embedding, Flatten, Activation, Dropout\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.python.client import device_lib\n",
        "warnings.filterwarnings('ignore')\n",
        "import random"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOson70GPOXb"
      },
      "source": [
        "import gc"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQsVybBH3VPw"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwtTusGxrmAE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "f2056007-70fd-4ac2-ac73-66edc598c5cc"
      },
      "source": [
        "import pandas as pd\n",
        "dataframe = pd.read_csv(\"/content/drive/MyDrive/dlresearch/Practice_Log_Demographics.csv\") \n",
        "dataframe.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>term</th>\n",
              "      <th>Final_Exam</th>\n",
              "      <th>Final_Exam_Std</th>\n",
              "      <th>Final_Exam_Percent</th>\n",
              "      <th>Final_Exam_0_1</th>\n",
              "      <th>Final_Exam_log_trans</th>\n",
              "      <th>Dropped</th>\n",
              "      <th>SNPSHT_RPT_DT</th>\n",
              "      <th>STDNT_SEX_CD</th>\n",
              "      <th>gender</th>\n",
              "      <th>birthYear</th>\n",
              "      <th>birthMonth</th>\n",
              "      <th>STDNT_ASIAN_IND</th>\n",
              "      <th>STDNT_BLACK_IND</th>\n",
              "      <th>STDNT_HWIAN_IND</th>\n",
              "      <th>STDNT_HSPNC_IND</th>\n",
              "      <th>STDNT_NTV_AMRCN_IND</th>\n",
              "      <th>STDNT_WHITE_IND</th>\n",
              "      <th>STDNT_ETHNC_GRP_CD</th>\n",
              "      <th>ethnicity</th>\n",
              "      <th>STDNT_MULTI_ETHNC_IND</th>\n",
              "      <th>STDNT_HSPNC_LATINO_IND</th>\n",
              "      <th>nativeEnglish</th>\n",
              "      <th>FIRST_US_PRMNNT_RES_PSTL_CD</th>\n",
              "      <th>FIRST_US_PRMNNT_RES_PSTL_5_CD</th>\n",
              "      <th>FRST_FRGN_PRMNNT_RES_CNTRY_CD</th>\n",
              "      <th>permanentCountry</th>\n",
              "      <th>STDNT_CTZN_STAT_CD</th>\n",
              "      <th>USCitizenship</th>\n",
              "      <th>STDNT_CTZN_CNTRY_1_CD</th>\n",
              "      <th>citizenship</th>\n",
              "      <th>STDNT_CTZN_CNTRY_2_CD</th>\n",
              "      <th>STDNT_CTZN_CNTRY_2_DES</th>\n",
              "      <th>international</th>\n",
              "      <th>FIRST_TERM_ATTND_CD</th>\n",
              "      <th>firstTerm</th>\n",
              "      <th>FIRST_TERM_ATTND_BEGIN_YR_MO</th>\n",
              "      <th>FIRST_TERM_ATTND_END_YR_MO</th>\n",
              "      <th>LAST_TERM_ATTND_CD</th>\n",
              "      <th>...</th>\n",
              "      <th>Freshman</th>\n",
              "      <th>Junior</th>\n",
              "      <th>Sophomore</th>\n",
              "      <th>Senior</th>\n",
              "      <th>Minors_1</th>\n",
              "      <th>Minors_2OrMore</th>\n",
              "      <th>athlete_1</th>\n",
              "      <th>honorsPro_1</th>\n",
              "      <th>program_Cat</th>\n",
              "      <th>LSA</th>\n",
              "      <th>programBusiness</th>\n",
              "      <th>programEngineering</th>\n",
              "      <th>programInformation</th>\n",
              "      <th>programOther</th>\n",
              "      <th>interaction_count</th>\n",
              "      <th>activeCode_count</th>\n",
              "      <th>codelens_count</th>\n",
              "      <th>mChoice_count</th>\n",
              "      <th>pageVideoViews_count</th>\n",
              "      <th>parsons_count</th>\n",
              "      <th>interaction_days</th>\n",
              "      <th>spacing</th>\n",
              "      <th>user_id.x</th>\n",
              "      <th>practice_count</th>\n",
              "      <th>practice_days</th>\n",
              "      <th>user_id.y</th>\n",
              "      <th>course_name.y</th>\n",
              "      <th>chapter_label</th>\n",
              "      <th>sub_chapter_label</th>\n",
              "      <th>question_name</th>\n",
              "      <th>i_interval</th>\n",
              "      <th>e_factor</th>\n",
              "      <th>q</th>\n",
              "      <th>trials_num</th>\n",
              "      <th>day's_available_flashcards</th>\n",
              "      <th>start_practice</th>\n",
              "      <th>end_practice</th>\n",
              "      <th>timezoneoffset</th>\n",
              "      <th>next_eligible_date</th>\n",
              "      <th>days_offset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>WN 2018</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.199159</td>\n",
              "      <td>89.2</td>\n",
              "      <td>0.883168</td>\n",
              "      <td>2.4681</td>\n",
              "      <td>0</td>\n",
              "      <td>09-FEB-21</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>1998</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Native</td>\n",
              "      <td>02052-3110</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Citzn</td>\n",
              "      <td>USA</td>\n",
              "      <td>United States</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>FA 2016</td>\n",
              "      <td>2016/09</td>\n",
              "      <td>2016/12</td>\n",
              "      <td>2270</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LS&amp;A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5913.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>636.0</td>\n",
              "      <td>1342.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>23.016143</td>\n",
              "      <td>148</td>\n",
              "      <td>493</td>\n",
              "      <td>44</td>\n",
              "      <td>148</td>\n",
              "      <td>UMSI106</td>\n",
              "      <td>Functions</td>\n",
              "      <td>DecodingaFunction</td>\n",
              "      <td>test_questionfunctions_3_3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "      <td>2018-02-10 12:03:08</td>\n",
              "      <td>2018-02-10 12:04:01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>WN 2018</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.199159</td>\n",
              "      <td>89.2</td>\n",
              "      <td>0.883168</td>\n",
              "      <td>2.4681</td>\n",
              "      <td>0</td>\n",
              "      <td>09-FEB-21</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>1998</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Native</td>\n",
              "      <td>02052-3110</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Citzn</td>\n",
              "      <td>USA</td>\n",
              "      <td>United States</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>FA 2016</td>\n",
              "      <td>2016/09</td>\n",
              "      <td>2016/12</td>\n",
              "      <td>2270</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LS&amp;A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5913.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>636.0</td>\n",
              "      <td>1342.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>23.016143</td>\n",
              "      <td>148</td>\n",
              "      <td>493</td>\n",
              "      <td>44</td>\n",
              "      <td>148</td>\n",
              "      <td>UMSI106</td>\n",
              "      <td>Tuples</td>\n",
              "      <td>UnpackingDictionaryItems</td>\n",
              "      <td>ee_ch09_05</td>\n",
              "      <td>1</td>\n",
              "      <td>1.40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-02-20 13:31:37</td>\n",
              "      <td>2018-02-20 13:31:41</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>WN 2018</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.199159</td>\n",
              "      <td>89.2</td>\n",
              "      <td>0.883168</td>\n",
              "      <td>2.4681</td>\n",
              "      <td>0</td>\n",
              "      <td>09-FEB-21</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>1998</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Native</td>\n",
              "      <td>02052-3110</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Citzn</td>\n",
              "      <td>USA</td>\n",
              "      <td>United States</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>FA 2016</td>\n",
              "      <td>2016/09</td>\n",
              "      <td>2016/12</td>\n",
              "      <td>2270</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LS&amp;A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5913.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>636.0</td>\n",
              "      <td>1342.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>23.016143</td>\n",
              "      <td>148</td>\n",
              "      <td>493</td>\n",
              "      <td>44</td>\n",
              "      <td>148</td>\n",
              "      <td>UMSI106</td>\n",
              "      <td>NestedData</td>\n",
              "      <td>DebuggingNestedData</td>\n",
              "      <td>ee_nested_data_011</td>\n",
              "      <td>15</td>\n",
              "      <td>2.50</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-03-16 14:09:57</td>\n",
              "      <td>2018-03-16 14:10:33</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>WN 2018</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.199159</td>\n",
              "      <td>89.2</td>\n",
              "      <td>0.883168</td>\n",
              "      <td>2.4681</td>\n",
              "      <td>0</td>\n",
              "      <td>09-FEB-21</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>1998</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Native</td>\n",
              "      <td>02052-3110</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Citzn</td>\n",
              "      <td>USA</td>\n",
              "      <td>United States</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>FA 2016</td>\n",
              "      <td>2016/09</td>\n",
              "      <td>2016/12</td>\n",
              "      <td>2270</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LS&amp;A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5913.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>636.0</td>\n",
              "      <td>1342.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>23.016143</td>\n",
              "      <td>148</td>\n",
              "      <td>493</td>\n",
              "      <td>44</td>\n",
              "      <td>148</td>\n",
              "      <td>UMSI106</td>\n",
              "      <td>SimplePythonData</td>\n",
              "      <td>FunctionCalls</td>\n",
              "      <td>exercise_functionCalls_1</td>\n",
              "      <td>16</td>\n",
              "      <td>2.60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>2018-01-30 14:15:49</td>\n",
              "      <td>2018-01-30 14:15:58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>WN 2018</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.199159</td>\n",
              "      <td>89.2</td>\n",
              "      <td>0.883168</td>\n",
              "      <td>2.4681</td>\n",
              "      <td>0</td>\n",
              "      <td>09-FEB-21</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>1998</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Native</td>\n",
              "      <td>02052-3110</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Citzn</td>\n",
              "      <td>USA</td>\n",
              "      <td>United States</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>FA 2016</td>\n",
              "      <td>2016/09</td>\n",
              "      <td>2016/12</td>\n",
              "      <td>2270</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LS&amp;A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5913.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>636.0</td>\n",
              "      <td>1342.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>23.016143</td>\n",
              "      <td>148</td>\n",
              "      <td>493</td>\n",
              "      <td>44</td>\n",
              "      <td>148</td>\n",
              "      <td>UMSI106</td>\n",
              "      <td>SimplePythonData</td>\n",
              "      <td>DataTypes</td>\n",
              "      <td>test_question2_1_2</td>\n",
              "      <td>59</td>\n",
              "      <td>2.10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>2018-03-13 14:22:54</td>\n",
              "      <td>2018-03-13 14:23:16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 760 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0     term  ...  next_eligible_date  days_offset\n",
              "0           1  WN 2018  ...                   0           23\n",
              "1           2  WN 2018  ...                   0           33\n",
              "2           3  WN 2018  ...                   0           57\n",
              "3           4  WN 2018  ...                   0           12\n",
              "4           5  WN 2018  ...                   0           54\n",
              "\n",
              "[5 rows x 760 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCbHnEp_Vir0"
      },
      "source": [
        "dataframe = dataframe.sort_values(by=['start_practice'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RvF7BvEr98o"
      },
      "source": [
        "FEATURES = ['Repeated', 'chapter_label', 'sub_chapter_label','question_name','user_id','term',\n",
        "            'STDNT_SEX_CD', \n",
        "            'NonNativeEnglish',\n",
        "            'White',\n",
        "            'Asian',\n",
        "            'WhiteOrAsian',\n",
        "            'Hispanic',\n",
        "            'AfricanAmerican',\n",
        "            'OtherEthnicities',\n",
        "            'NonWhiteOrAsian',\n",
        "            'STDNT_CTZN_STAT_CD', 'international', \n",
        "            'gradingType',\n",
        "            'birthYear',\n",
        "            'exclClassCumGPA',\n",
        "            'Freshman',\n",
        "            'Junior',\n",
        "            'Sophomore',\n",
        "            'Senior',\n",
        "            'termCreditsGPA',\n",
        "            'termCreditsNoGPA',\n",
        "            'athlete_1',\n",
        "            'honorsPro',\n",
        "            'LSA', 'programBusiness', 'programEngineering', \n",
        "            'programInformation', 'programOther',\n",
        "            'HSCalculusTaken', \n",
        "            'highSchoolGPA', \n",
        "            'majorsCount', 'minorsCount',\n",
        "            'PREV_TERM_CUM_GPA',\n",
        "            'classGraded', 'classHonors', \n",
        "            'Pass_Fail', \n",
        "            'parentsGraduateEdu',  'minorityGroup', \n",
        "            'q',\n",
        "            'available_flashcards', \n",
        "            'start_practice', \n",
        "            'end_practice',\n",
        "            'days_offset']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn341X7lSztl"
      },
      "source": [
        "dataframe = dataframe[(dataframe[\"term\"] == \"WN 2018\") |\n",
        "                      (dataframe[\"term\"] == \"FA 2018\") |\n",
        "                      (dataframe[\"term\"] == \"WN 2019\") |\n",
        "                      (dataframe[\"term\"] == \"FA 2019\") |\n",
        "                      (dataframe[\"term\"] == \"WN 2020\") |\n",
        "                      (dataframe[\"term\"] == \"FA 2020\")]\n",
        "dataframe['available_flashcards'] = dataframe[\"day's_available_flashcards\"][:]\n",
        "dataframe = dataframe.drop([\"day's_available_flashcards\"], axis=1)\n",
        "dataframe['user_id'] = dataframe[\"user_id.y\"][:]\n",
        "dataframe = dataframe.drop([\"user_id.y\"], axis=1)\n",
        "dataframe = dataframe.drop([\"user_id.x\"], axis=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_BRggOjbBjN",
        "outputId": "810eca28-8438-4f48-9d99-66b046640c11"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXM7MlonVeRL",
        "outputId": "c1ae81fe-9c91-452f-b901-544841d7d46e"
      },
      "source": [
        "dataframe = dataframe[FEATURES]\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WAFv5z3A6fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07042d0-e180-46e8-ccda-e3a54a837ffc"
      },
      "source": [
        "dataframe = dataframe.fillna(0)\n",
        "dataframe['answer_correct'] = np.where(dataframe['q']>=4, 1, 0)\n",
        "dataframe['answer_correct'].mean()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5326395552433993"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRKqF_0IXZgj"
      },
      "source": [
        "dataframe['term'] = dataframe['term'].astype('category')\n",
        "dataframe['user_id'] = dataframe['user_id'].astype(int)\n",
        "dataframe['user_id'] = dataframe['user_id'].astype(str)\n",
        "dataframe['user_id'] = dataframe['term'].str.cat(dataframe['user_id'], sep=':')\n",
        "dataframe['user_id'] = dataframe['user_id'].astype('category')\n",
        "dataframe['chapter_label'] = dataframe['chapter_label'].astype('category')\n",
        "dataframe['sub_chapter_label'] = dataframe['sub_chapter_label'].astype('category')\n",
        "dataframe['question_name'] = dataframe['question_name'].astype('category')\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVl_067GWkDe"
      },
      "source": [
        "dataframe['prev_time_elapsed'] = None\n",
        "dataframe['time_lag'] = None\n",
        "dataframe['time_lag'] = dataframe['time_lag'].astype(np.float)\n",
        "dataframe['prev_time_elapsed'] = dataframe['prev_time_elapsed'].astype(np.float)\n",
        "dataframe.start_practice = pd.to_datetime(dataframe.start_practice, format='%Y-%m-%d %H:%M:%S')\n",
        "dataframe.end_practice = pd.to_datetime(dataframe.end_practice, format='%Y-%m-%d %H:%M:%S')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xrkcn9VhWETm",
        "outputId": "8e3db4bd-51cb-4c0a-8a33-1db0982a9190"
      },
      "source": [
        "from tqdm import tqdm\n",
        "for user in tqdm(dataframe['user_id'].unique()):\n",
        "    tmp_user = dataframe[dataframe['user_id']==user]\n",
        "    tmp_time_elapsed = tmp_user.end_practice.apply(lambda a: a.timestamp()) - tmp_user.start_practice.apply(lambda a: a.timestamp())\n",
        "    #shifting time elapsed by one\n",
        "    #so that time_elapsed row for each question\n",
        "    #would refer to the time that user took to answer\n",
        "    #previous question\n",
        "    tmp_time_elapsed = np.insert(np.array(tmp_time_elapsed[:-1]), 0, -1., axis=0)\n",
        "    indices = tmp_user.index\n",
        "    start_row = indices[0]\n",
        "    dataframe['time_lag'].iloc[start_row] = -1\n",
        "    time_substrahend = tmp_user.start_practice.iloc[:-1]\n",
        "    time_substrahend = time_substrahend.apply(lambda a: a.timestamp())\n",
        "    time_substrahend = np.array(time_substrahend)\n",
        "\n",
        "    time_minuend = tmp_user.start_practice.iloc[1:]\n",
        "    time_minuend = time_minuend.apply(lambda a: a.timestamp())\n",
        "    time_minuend = np.array(time_minuend)\n",
        "    \n",
        "    dataframe['prev_time_elapsed'].iloc[indices] = tmp_time_elapsed\n",
        "    dataframe['time_lag'].iloc[indices[1:]] = time_minuend - time_substrahend"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 887/887 [00:11<00:00, 77.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S96YQhUGZ-Au",
        "outputId": "f1180f2b-f431-4a31-aaff-2c74d09dc5b9"
      },
      "source": [
        "dataframe  = dataframe[dataframe['Repeated'] == ' ']\n",
        "gc.collect()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t8yZ0XvWwzz"
      },
      "source": [
        "dataframe.drop(columns=['end_practice'], inplace=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E42raRIrYYXz"
      },
      "source": [
        "dataframe['term_value'] = [int(ele[3:]) for ele in dataframe['term']]\n",
        "dataframe['age'] = dataframe['term_value'] - dataframe['birthYear']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcMVQXueYnMh"
      },
      "source": [
        "dataframe.drop(columns=['term_value', 'birthYear'], inplace=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjRF5szkcQE0"
      },
      "source": [
        "new_minors_count = []\n",
        "for i in dataframe['minorsCount']:\n",
        "  if i == 0 or i == '0':\n",
        "    new_minors_count.append(0)\n",
        "  elif i == '1 Minor':\n",
        "    new_minors_count.append(1)\n",
        "  else:\n",
        "    new_minors_count.append(2)\n",
        "\n",
        "dataframe['minorsCount'] = new_minors_count"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VdzvJSw0KUU",
        "outputId": "31043674-c51f-4b9d-9a8a-6c40576ebc5b"
      },
      "source": [
        "print(\"we have \", dataframe['user_id'].nunique(),\" users in total.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we have  881  users in total.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUaGk9L9g13t"
      },
      "source": [
        "for category in ['term','chapter_label', 'sub_chapter_label', 'question_name']:\n",
        "  dataframe[category] =  dataframe[category].cat.codes\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaJDiFgofl6P",
        "outputId": "b4ec5f7a-f3dc-419f-fed1-f53a4dd52c47"
      },
      "source": [
        "gc.collect()\n",
        "dataframe.info()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 501498 entries, 314373 to 151634\n",
            "Data columns (total 50 columns):\n",
            " #   Column                Non-Null Count   Dtype         \n",
            "---  ------                --------------   -----         \n",
            " 0   Repeated              501498 non-null  object        \n",
            " 1   chapter_label         501498 non-null  int8          \n",
            " 2   sub_chapter_label     501498 non-null  int16         \n",
            " 3   question_name         501498 non-null  int16         \n",
            " 4   user_id               501498 non-null  category      \n",
            " 5   term                  501498 non-null  int8          \n",
            " 6   STDNT_SEX_CD          501498 non-null  int64         \n",
            " 7   NonNativeEnglish      501498 non-null  int64         \n",
            " 8   White                 501498 non-null  int64         \n",
            " 9   Asian                 501498 non-null  int64         \n",
            " 10  WhiteOrAsian          501498 non-null  int64         \n",
            " 11  Hispanic              501498 non-null  int64         \n",
            " 12  AfricanAmerican       501498 non-null  int64         \n",
            " 13  OtherEthnicities      501498 non-null  int64         \n",
            " 14  NonWhiteOrAsian       501498 non-null  int64         \n",
            " 15  STDNT_CTZN_STAT_CD    501498 non-null  int64         \n",
            " 16  international         501498 non-null  int64         \n",
            " 17  gradingType           501498 non-null  object        \n",
            " 18  exclClassCumGPA       501498 non-null  float64       \n",
            " 19  Freshman              501498 non-null  int64         \n",
            " 20  Junior                501498 non-null  int64         \n",
            " 21  Sophomore             501498 non-null  int64         \n",
            " 22  Senior                501498 non-null  int64         \n",
            " 23  termCreditsGPA        501498 non-null  float64       \n",
            " 24  termCreditsNoGPA      501498 non-null  float64       \n",
            " 25  athlete_1             501498 non-null  int64         \n",
            " 26  honorsPro             501498 non-null  int64         \n",
            " 27  LSA                   501498 non-null  int64         \n",
            " 28  programBusiness       501498 non-null  int64         \n",
            " 29  programEngineering    501498 non-null  int64         \n",
            " 30  programInformation    501498 non-null  int64         \n",
            " 31  programOther          501498 non-null  int64         \n",
            " 32  HSCalculusTaken       501498 non-null  int64         \n",
            " 33  highSchoolGPA         501498 non-null  float64       \n",
            " 34  majorsCount           501498 non-null  int64         \n",
            " 35  minorsCount           501498 non-null  int64         \n",
            " 36  PREV_TERM_CUM_GPA     501498 non-null  float64       \n",
            " 37  classGraded           501498 non-null  int64         \n",
            " 38  classHonors           501498 non-null  int64         \n",
            " 39  Pass_Fail             501498 non-null  int64         \n",
            " 40  parentsGraduateEdu    501498 non-null  int64         \n",
            " 41  minorityGroup         501498 non-null  int64         \n",
            " 42  q                     501498 non-null  int64         \n",
            " 43  available_flashcards  501498 non-null  int64         \n",
            " 44  start_practice        501498 non-null  datetime64[ns]\n",
            " 45  days_offset           501498 non-null  int64         \n",
            " 46  answer_correct        501498 non-null  int64         \n",
            " 47  prev_time_elapsed     501498 non-null  float64       \n",
            " 48  time_lag              501498 non-null  float64       \n",
            " 49  age                   501498 non-null  int64         \n",
            "dtypes: category(1), datetime64[ns](1), float64(7), int16(2), int64(35), int8(2), object(2)\n",
            "memory usage: 179.9+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkyvcYvlhliG"
      },
      "source": [
        "NUMERIC_FEATURE =  ['age',\n",
        "            'exclClassCumGPA',\n",
        "            'termCreditsGPA',\n",
        "            'termCreditsNoGPA',\n",
        "            'highSchoolGPA', \n",
        "            'majorsCount', 'minorsCount',\n",
        "            'PREV_TERM_CUM_GPA',\n",
        "            'available_flashcards', \n",
        "            'days_offset', \n",
        "            'prev_time_elapsed',\n",
        "             'time_lag']\n",
        "\n",
        "for f in NUMERIC_FEATURE:\n",
        "  m = dataframe[f].mean()\n",
        "  std = dataframe[f].std()\n",
        "  dataframe[f] = (dataframe[f] - m)/std"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo2OH-d5kWp1"
      },
      "source": [
        "FEATURE_TRANS =  ['answer_correct', 'chapter_label', 'sub_chapter_label','question_name','user_id','term',\n",
        "                  'STDNT_SEX_CD', \n",
        "                    'White','Asian','NonWhiteOrAsian',\n",
        "            'STDNT_CTZN_STAT_CD', 'international', \n",
        "            'age',\n",
        "            'exclClassCumGPA',\n",
        "           'Freshman',\n",
        "            'Junior',\n",
        "            'Sophomore',\n",
        "            'Senior',\n",
        "            'termCreditsGPA',\n",
        "            'termCreditsNoGPA',\n",
        "            'athlete_1',\n",
        "            'honorsPro',\n",
        "            'LSA', 'programBusiness', 'programEngineering', \n",
        "            'programInformation', 'programOther',\n",
        "            'HSCalculusTaken', \n",
        "            'highSchoolGPA', \n",
        "\n",
        "            'majorsCount', 'minorsCount',\n",
        "\n",
        "            'PREV_TERM_CUM_GPA',\n",
        "            'classGraded', 'classHonors', \n",
        "            'Pass_Fail', \n",
        "            'parentsGraduateEdu',  'minorityGroup', \n",
        "            'available_flashcards', \n",
        "            'days_offset', 'prev_time_elapsed',\n",
        "             'time_lag']\n",
        "grouped_data = dataframe[FEATURE_TRANS].groupby(['user_id']).apply(lambda r: (\n",
        "                r['answer_correct'],\n",
        "                r['term'],\n",
        "                r['chapter_label'],\n",
        "                r['sub_chapter_label'],\n",
        "                r['question_name'],\n",
        "                np.array([r['STDNT_SEX_CD'],r['STDNT_CTZN_STAT_CD'], r['international'], \n",
        "                  r['White'],r['Asian'],r['NonWhiteOrAsian'],\n",
        "                 r['age'],r['exclClassCumGPA'],\n",
        "                r['Freshman'], r['Junior'], r['Sophomore'], r['Senior'],\n",
        "                r['termCreditsGPA'], r['termCreditsNoGPA'],\n",
        "                r['athlete_1'], r['honorsPro'],\n",
        "                r['LSA'], r['programBusiness'], r['programEngineering'], \n",
        "                r['programInformation'], r['programOther'],\n",
        "                r['HSCalculusTaken'],  r['highSchoolGPA'], \n",
        "                r['majorsCount'], r['minorsCount'],\n",
        "                r['PREV_TERM_CUM_GPA'], \n",
        "                r['parentsGraduateEdu'], r['minorityGroup'],\n",
        "                r['available_flashcards'],\n",
        "                r['days_offset'],\n",
        "                r['prev_time_elapsed'],\n",
        "                r['time_lag']\n",
        "              ]).transpose()\n",
        "                ))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXLqQnjrlp_4",
        "outputId": "657c43a0-738c-4aea-bab7-d5e12a7796c6"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYGQasdTrmP0",
        "outputId": "1dbc6deb-52ce-4389-f8a4-1cb6d8709d0f"
      },
      "source": [
        "toRemove = []\n",
        "for index in grouped_data.index:\n",
        "  # print(grouped_data[index][0])\n",
        "  # break\n",
        "  if len(grouped_data[index][0]) <= 10:\n",
        "    print(index)\n",
        "    toRemove.append(index)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FA 2018:231\n",
            "FA 2018:248\n",
            "FA 2019:227\n",
            "WN 2019:105\n",
            "WN 2019:25\n",
            "WN 2019:39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "561M7y0ZsSBC"
      },
      "source": [
        "grouped_data = grouped_data.drop(index=toRemove)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqw51WilmHQ4",
        "outputId": "6dd1af24-eb43-4b51-b215-171cbaaefc79"
      },
      "source": [
        "num_interact = 0\n",
        "count  = 0\n",
        "max_num_interact = 0\n",
        "for g in grouped_data:\n",
        "  if len(g[0])<10:\n",
        "    print(len(g[0]))\n",
        "  if len(g[0]) > max_num_interact:\n",
        "    max_num_interact = len(g[0])\n",
        "  num_interact += len(g[0])\n",
        "  count += 1\n",
        "print(num_interact/count)\n",
        "print(max_num_interact)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "569.2372304199773\n",
            "1348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kZqV9siDyNb"
      },
      "source": [
        "#SETTINGS -> can be modified at any time\n",
        "MAXLENGTH = 500\n",
        "EMBEDDING_DIM = 128\n",
        "DENSE_NEURON = 16\n",
        "LSTM_NEURON = 32"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MksD1JizpPn"
      },
      "source": [
        "FEATURES_SIZE = 39\n",
        "CHAPTER_SIZE = 38\n",
        "SUB_CHAPTER_SIZE = 222\n",
        "QUESTION_SIZE = 1065"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY3Thp6d0NaT"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class SPACE_DATASET(Dataset):\n",
        "    def __init__(self, data, maxlength = 100):\n",
        "        super(SPACE_DATASET, self).__init__()\n",
        "        self.maxlength = maxlength\n",
        "        self.data = data\n",
        "        self.users = list()\n",
        "        for user in data.index:\n",
        "            self.users.append(user)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "    \n",
        "    def __getitem__(self, ix):\n",
        "        user = self.users[ix]\n",
        "        user = user\n",
        "        target, term, ch_label, sub_ch_label, ques_name, features = self.data[user]\n",
        "        \n",
        "        #0s should be used as padding values\n",
        "        ori_target = target.values \n",
        "        term = term.values\n",
        "        ch_label = ch_label.values + 1\n",
        "        sub_ch_label = sub_ch_label.values +1\n",
        "        ques_name = ques_name.values + 1\n",
        "        \n",
        "        n = len(ch_label)\n",
        "\n",
        "        # one hot for term\n",
        "        term_encode = [0]*6\n",
        "        term_encode[term[0]] = 1\n",
        "        shifted_target= []\n",
        "\n",
        "        \n",
        "        # get module ids and user interaction informations in the previous n modules\n",
        "        if n > self.maxlength:\n",
        "          ch_label = ch_label[-self.maxlength:]\n",
        "          sub_ch_label = sub_ch_label[-self.maxlength:]\n",
        "          ques_name = ques_name[-self.maxlength:]\n",
        "          features = features[-self.maxlength:]\n",
        "          target = ori_target[-self.maxlength:]\n",
        "          shifted_target = ori_target[ (-self.maxlength - 1) :-1]\n",
        "        else:\n",
        "          ch_label = [0]*(self.maxlength - n)+list(ch_label[:])\n",
        "          sub_ch_label = [0]*(self.maxlength - n)+list(sub_ch_label[:])\n",
        "          ques_name = [0]*(self.maxlength - n)+list(ques_name[:])\n",
        "          features = [[0]*len(features[0])]*(self.maxlength  - n)+list(features[:])\n",
        "          target = [-1]*(self.maxlength - n) + list(ori_target[:])\n",
        "          shifted_target = [-1]*(self.maxlength + 1 - n) + list(ori_target[:-1])\n",
        "\n",
        "        new_features = []\n",
        "        count = 0\n",
        "        for f in features:\n",
        "          temp = list(f)\n",
        "          temp.extend(term_encode)\n",
        "          temp.append(shifted_target[count]) #uncomment this line for include previous response feature\n",
        "          new_features.append(temp)\n",
        "          \n",
        "          count += 1\n",
        "        \n",
        "        features = new_features\n",
        "        \n",
        "        return ch_label,sub_ch_label,ques_name,features,target"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xc90-aLzxat"
      },
      "source": [
        "## KFOLD - GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzJrljnjzypP",
        "outputId": "756b7ecd-2c8d-4d11-bd25-d5f61ce48ae0"
      },
      "source": [
        "import torch\n",
        "X = np.array(grouped_data.keys())\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "train_losses = list()\n",
        "train_aucs = list()\n",
        "val_losses = list()\n",
        "val_aucs = list()\n",
        "train_eval = list()\n",
        "test_eval = list()\n",
        "for train, test in kfold.split(X):\n",
        "    users_train, users_test =  X[train], X[test]\n",
        "    n = len(users_test)//2\n",
        "    users_test, users_val = users_test[:n], users_test[n: ]\n",
        "    train_data_space = SPACE_DATASET(grouped_data[users_train], MAXLENGTH)\n",
        "    val_data_space = SPACE_DATASET(grouped_data[users_val], MAXLENGTH)\n",
        "    test_data_space = SPACE_DATASET(grouped_data[users_test], MAXLENGTH)\n",
        "    #construct training input\n",
        "    train_chapter=[]\n",
        "    train_sub_chapter=[]\n",
        "    train_question = []\n",
        "    train_features=[]\n",
        "    train_labels=[]\n",
        "    for i in range(len(users_train)):\n",
        "        user = train_data_space.__getitem__(i)\n",
        "        train_chapter.append(user[0])\n",
        "        train_sub_chapter.append(user[1]) \n",
        "        train_question.append(user[2])\n",
        "        train_features.append(user[3])\n",
        "        train_labels.append(user[4])\n",
        "\n",
        "\n",
        "    train_chapter = np.array(train_chapter)\n",
        "    train_sub_chapter = np.array(train_sub_chapter)\n",
        "    train_question = np.array(train_question)\n",
        "    train_features = np.array(train_features)\n",
        "    train_labels= np.array(train_labels)[..., np.newaxis]\n",
        "\n",
        "    val_chapter=[]\n",
        "    val_sub_chapter=[]\n",
        "    val_question = []\n",
        "    val_features=[]\n",
        "    #val_term=[]\n",
        "    val_labels=[]\n",
        "    for i in range(len(users_val)):\n",
        "        user = val_data_space.__getitem__(i)\n",
        "        val_chapter.append(user[0])\n",
        "        val_sub_chapter.append(user[1]) \n",
        "        val_question.append(user[2])\n",
        "        val_features.append(user[3])\n",
        "        val_labels.append(user[4])\n",
        "\n",
        "\n",
        "    val_chapter = np.array(val_chapter)\n",
        "    val_sub_chapter = np.array(val_sub_chapter)\n",
        "    val_features = np.array(val_features)\n",
        "    val_question = np.array(val_question)\n",
        "    val_labels= np.array(val_labels)[..., np.newaxis]\n",
        "\n",
        "    test_chapter=[]\n",
        "    test_sub_chapter=[]\n",
        "    test_features=[]\n",
        "    test_question=[]\n",
        "    test_labels=[]\n",
        "    for i in range(len(users_test)):\n",
        "        user = test_data_space.__getitem__(i)\n",
        "        test_chapter.append(user[0])\n",
        "        test_sub_chapter.append(user[1]) \n",
        "        test_question.append(user[2])\n",
        "        test_features.append(user[3])\n",
        "        test_labels.append(user[4])\n",
        "        #test_term.append(user[4])\n",
        "\n",
        "\n",
        "    test_chapter = np.array(test_chapter)\n",
        "    test_sub_chapter = np.array(test_sub_chapter)\n",
        "    test_features = np.array(test_features)\n",
        "    test_question = np.array(test_question)\n",
        "    test_labels= np.array(test_labels)[..., np.newaxis]\n",
        "\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    acc = tf.keras.metrics.Accuracy()\n",
        "    auc = tf.keras.metrics.AUC()\n",
        "\n",
        "    def masked_bce(y_true, y_pred):\n",
        "      flat_pred = y_pred\n",
        "      flat_ground_truth = y_true\n",
        "      label_mask = tf.math.not_equal(flat_ground_truth, -1)\n",
        "      return bce(flat_ground_truth, flat_pred, sample_weight=label_mask)\n",
        "\n",
        "    def masked_acc(y_true, y_pred):\n",
        "      flat_pred = y_pred\n",
        "      flat_ground_truth = y_true\n",
        "      flat_pred = (flat_pred >= 0.5)\n",
        "      label_mask = tf.math.not_equal(flat_ground_truth, -1)\n",
        "      return acc(flat_ground_truth, flat_pred, sample_weight=label_mask)\n",
        "\n",
        "    def masked_auc(y_true, y_pred):\n",
        "      flat_pred = y_pred\n",
        "      flat_ground_truth = y_true\n",
        "      label_mask = tf.math.not_equal(flat_ground_truth, -1)\n",
        "      return auc(flat_ground_truth, flat_pred, sample_weight=label_mask)\n",
        "\n",
        "    # input layer\n",
        "    input_chap = tf.keras.Input(shape=(MAXLENGTH))\n",
        "    input_sub_chap = tf.keras.Input(shape=(MAXLENGTH))\n",
        "    input_ques =  tf.keras.Input(shape=(MAXLENGTH))\n",
        "    input_features = tf.keras.Input(shape=(MAXLENGTH, FEATURES_SIZE))\n",
        "\n",
        "    # embedding layer for categorical features\n",
        "    embedding_chap = Embedding(input_dim = CHAPTER_SIZE, output_dim = EMBEDDING_DIM)(input_chap)\n",
        "    embedding_sub_chap = Embedding(input_dim = SUB_CHAPTER_SIZE, output_dim = EMBEDDING_DIM)(input_sub_chap) \n",
        "    embedding_ques = Embedding(input_dim = QUESTION_SIZE, output_dim = EMBEDDING_DIM)(input_ques)       \n",
        "\n",
        "    # dense features\n",
        "    dense_features = Dense(EMBEDDING_DIM,input_shape = (None, MAXLENGTH))(input_features)\n",
        "\n",
        "\n",
        "\n",
        "    # lstm\n",
        "    lstm_chap = GRU(LSTM_NEURON, input_shape = (None, EMBEDDING_DIM),return_sequences = True)(embedding_chap)\n",
        "    lstm_sub_chap = GRU(LSTM_NEURON, input_shape = (None, EMBEDDING_DIM),return_sequences = True)(embedding_sub_chap)\n",
        "    lstm_ques = GRU(LSTM_NEURON, input_shape = (None, EMBEDDING_DIM),return_sequences = True)(embedding_ques)\n",
        "    lstm_features = GRU(LSTM_NEURON, input_shape = (None, EMBEDDING_DIM),return_sequences = True)(dense_features)\n",
        "\n",
        "    lstm_output = tf.concat([lstm_chap, lstm_sub_chap, lstm_ques,lstm_features], axis = 2)\n",
        "\n",
        "    # flatten = Flatten()(cancat_layer)\n",
        "    dense1 = Dense(256, input_shape = (None, 4*EMBEDDING_DIM), activation='relu')(lstm_output)\n",
        "    dropout1 = Dropout(0.1)(dense1)\n",
        "    dense2 = Dense(64, input_shape = (None, 256), activation='relu')(dropout1)\n",
        "    dropout2 = Dropout(0.1)(dense2)\n",
        "    pred = Dense(1, input_shape = (None, 64), activation='sigmoid')(dropout2)\n",
        "\n",
        "\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs=[input_chap, input_sub_chap,input_ques, input_features],\n",
        "        outputs=pred,\n",
        "        name='gru_model'\n",
        "    )\n",
        "\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    opt_adam = Adam(learning_rate = 0.005)\n",
        "    model.compile(\n",
        "        optimizer=opt_adam,\n",
        "        loss= masked_bce,\n",
        "        metrics = [masked_acc, masked_auc]\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "      [train_chapter, train_sub_chapter, train_question, train_features],\n",
        "      train_labels,\n",
        "      batch_size = 64,\n",
        "      epochs = 100,\n",
        "      validation_data=([val_chapter, val_sub_chapter, val_question, val_features], val_labels),\n",
        "      callbacks=[callback]\n",
        "    )\n",
        "    val_losses.append(list(history.history['val_loss']))\n",
        "    train_losses.append(list(history.history['loss']))\n",
        "    val_aucs.append(list(history.history['val_masked_auc']))\n",
        "    train_aucs.append(list(history.history['masked_auc']))\n",
        "    train_score = model.evaluate([train_chapter, train_sub_chapter, train_question, train_features], train_labels)\n",
        "    train_eval.append(train_score)\n",
        "    test_score = model.evaluate([test_chapter, test_sub_chapter, test_question, test_features], test_labels)\n",
        "    test_eval.append(test_score)\n",
        "    print(\"Test: \", test_score)\n",
        "    def reset_weights(model):\n",
        "      for layer in model.layers: \n",
        "        if isinstance(layer, tf.keras.Model):\n",
        "          reset_weights(layer)\n",
        "          continue\n",
        "        for k, initializer in layer.__dict__.items():\n",
        "          if \"initializer\" not in k:\n",
        "            continue\n",
        "          # find the corresponding variable\n",
        "          var = getattr(layer, k.replace(\"_initializer\", \"\"))\n",
        "          var.assign(initializer(var.shape, var.dtype))\n",
        "    reset_weights(model)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 38s 262ms/step - loss: 0.6244 - masked_acc: 0.5503 - masked_auc: 0.5584 - val_loss: 0.5042 - val_masked_acc: 0.6363 - val_masked_auc: 0.6825\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.5073 - masked_acc: 0.6566 - masked_auc: 0.7110 - val_loss: 0.4613 - val_masked_acc: 0.6863 - val_masked_auc: 0.7506\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 1s 98ms/step - loss: 0.4676 - masked_acc: 0.6950 - masked_auc: 0.7613 - val_loss: 0.4295 - val_masked_acc: 0.7098 - val_masked_auc: 0.7794\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.4536 - masked_acc: 0.7147 - masked_auc: 0.7850 - val_loss: 0.4216 - val_masked_acc: 0.7235 - val_masked_auc: 0.7953\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.4514 - masked_acc: 0.7268 - masked_auc: 0.7992 - val_loss: 0.4198 - val_masked_acc: 0.7326 - val_masked_auc: 0.8058\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.4450 - masked_acc: 0.7348 - masked_auc: 0.8082 - val_loss: 0.4229 - val_masked_acc: 0.7390 - val_masked_auc: 0.8128\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.4437 - masked_acc: 0.7405 - masked_auc: 0.8144 - val_loss: 0.4113 - val_masked_acc: 0.7441 - val_masked_auc: 0.8184\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.4335 - masked_acc: 0.7454 - masked_auc: 0.8199 - val_loss: 0.4070 - val_masked_acc: 0.7482 - val_masked_auc: 0.8231\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 1s 98ms/step - loss: 0.4278 - masked_acc: 0.7494 - masked_auc: 0.8244 - val_loss: 0.4068 - val_masked_acc: 0.7516 - val_masked_auc: 0.8269\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 1s 97ms/step - loss: 0.4278 - masked_acc: 0.7525 - masked_auc: 0.8279 - val_loss: 0.4067 - val_masked_acc: 0.7544 - val_masked_auc: 0.8302\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.4285 - masked_acc: 0.7552 - masked_auc: 0.8311 - val_loss: 0.4043 - val_masked_acc: 0.7569 - val_masked_auc: 0.8330\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.4227 - masked_acc: 0.7577 - masked_auc: 0.8338 - val_loss: 0.4055 - val_masked_acc: 0.7591 - val_masked_auc: 0.8354\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.4195 - masked_acc: 0.7598 - masked_auc: 0.8362 - val_loss: 0.4051 - val_masked_acc: 0.7611 - val_masked_auc: 0.8377\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.4189 - masked_acc: 0.7617 - masked_auc: 0.8384 - val_loss: 0.4047 - val_masked_acc: 0.7629 - val_masked_auc: 0.8396\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.4068 - masked_acc: 0.7635 - masked_auc: 0.8403 - val_loss: 0.4138 - val_masked_acc: 0.7645 - val_masked_auc: 0.8414\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.4115 - masked_acc: 0.7650 - masked_auc: 0.8419 - val_loss: 0.4086 - val_masked_acc: 0.7659 - val_masked_auc: 0.8429\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.4168 - masked_acc: 0.7663 - masked_auc: 0.8433 - val_loss: 0.4087 - val_masked_acc: 0.7673 - val_masked_auc: 0.8445\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.4028 - masked_acc: 0.7678 - masked_auc: 0.8450 - val_loss: 0.4091 - val_masked_acc: 0.7686 - val_masked_auc: 0.8459\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.4102 - masked_acc: 0.7690 - masked_auc: 0.8463 - val_loss: 0.4108 - val_masked_acc: 0.7698 - val_masked_auc: 0.8472\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.4065 - masked_acc: 0.7702 - masked_auc: 0.8476 - val_loss: 0.4100 - val_masked_acc: 0.7710 - val_masked_auc: 0.8485\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.3982 - masked_acc: 0.7714 - masked_auc: 0.8489 - val_loss: 0.4101 - val_masked_acc: 0.7721 - val_masked_auc: 0.8498\n",
            "22/22 [==============================] - 2s 33ms/step - loss: 0.3936 - masked_acc: 0.7727 - masked_auc: 0.8504\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 0.4555 - masked_acc: 0.7733 - masked_auc: 0.8510\n",
            "Test:  [0.4554729461669922, 0.7732813954353333, 0.8510103225708008]\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 8s 234ms/step - loss: 0.6156 - masked_acc: 0.5457 - masked_auc: 0.5499 - val_loss: 0.5169 - val_masked_acc: 0.6314 - val_masked_auc: 0.6804\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.5128 - masked_acc: 0.6528 - masked_auc: 0.7095 - val_loss: 0.4784 - val_masked_acc: 0.6808 - val_masked_auc: 0.7466\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.4838 - masked_acc: 0.6891 - masked_auc: 0.7565 - val_loss: 0.4656 - val_masked_acc: 0.7023 - val_masked_auc: 0.7720\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 0.4673 - masked_acc: 0.7075 - masked_auc: 0.7778 - val_loss: 0.4460 - val_masked_acc: 0.7164 - val_masked_auc: 0.7879\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.4571 - masked_acc: 0.7198 - masked_auc: 0.7916 - val_loss: 0.4492 - val_masked_acc: 0.7255 - val_masked_auc: 0.7979\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.4570 - masked_acc: 0.7277 - masked_auc: 0.8003 - val_loss: 0.4374 - val_masked_acc: 0.7322 - val_masked_auc: 0.8052\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4550 - masked_acc: 0.7339 - masked_auc: 0.8068 - val_loss: 0.4336 - val_masked_acc: 0.7373 - val_masked_auc: 0.8102\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.4488 - masked_acc: 0.7387 - masked_auc: 0.8117 - val_loss: 0.4335 - val_masked_acc: 0.7414 - val_masked_auc: 0.8147\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4475 - masked_acc: 0.7424 - masked_auc: 0.8158 - val_loss: 0.4347 - val_masked_acc: 0.7440 - val_masked_auc: 0.8176\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 1s 107ms/step - loss: 0.4404 - masked_acc: 0.7450 - masked_auc: 0.8186 - val_loss: 0.4340 - val_masked_acc: 0.7468 - val_masked_auc: 0.8207\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 0.4440 - masked_acc: 0.7474 - masked_auc: 0.8215 - val_loss: 0.4282 - val_masked_acc: 0.7490 - val_masked_auc: 0.8232\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4420 - masked_acc: 0.7497 - masked_auc: 0.8239 - val_loss: 0.4242 - val_masked_acc: 0.7511 - val_masked_auc: 0.8255\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4401 - masked_acc: 0.7517 - masked_auc: 0.8261 - val_loss: 0.4261 - val_masked_acc: 0.7530 - val_masked_auc: 0.8275\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4414 - masked_acc: 0.7534 - masked_auc: 0.8280 - val_loss: 0.4268 - val_masked_acc: 0.7544 - val_masked_auc: 0.8291\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4336 - masked_acc: 0.7549 - masked_auc: 0.8297 - val_loss: 0.4252 - val_masked_acc: 0.7559 - val_masked_auc: 0.8308\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 109ms/step - loss: 0.4343 - masked_acc: 0.7564 - masked_auc: 0.8313 - val_loss: 0.4239 - val_masked_acc: 0.7573 - val_masked_auc: 0.8324\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 109ms/step - loss: 0.4361 - masked_acc: 0.7576 - masked_auc: 0.8327 - val_loss: 0.4221 - val_masked_acc: 0.7585 - val_masked_auc: 0.8337\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 0.4244 - masked_acc: 0.7589 - masked_auc: 0.8342 - val_loss: 0.4215 - val_masked_acc: 0.7597 - val_masked_auc: 0.8350\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 107ms/step - loss: 0.4269 - masked_acc: 0.7600 - masked_auc: 0.8353 - val_loss: 0.4231 - val_masked_acc: 0.7607 - val_masked_auc: 0.8361\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4329 - masked_acc: 0.7610 - masked_auc: 0.8364 - val_loss: 0.4331 - val_masked_acc: 0.7615 - val_masked_auc: 0.8370\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4317 - masked_acc: 0.7618 - masked_auc: 0.8373 - val_loss: 0.4317 - val_masked_acc: 0.7624 - val_masked_auc: 0.8378\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 0.4299 - masked_acc: 0.7626 - masked_auc: 0.8381 - val_loss: 0.4251 - val_masked_acc: 0.7631 - val_masked_auc: 0.8386\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4245 - masked_acc: 0.7633 - masked_auc: 0.8389 - val_loss: 0.4224 - val_masked_acc: 0.7639 - val_masked_auc: 0.8395\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4217 - masked_acc: 0.7642 - masked_auc: 0.8399 - val_loss: 0.4279 - val_masked_acc: 0.7646 - val_masked_auc: 0.8404\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4180 - masked_acc: 0.7649 - masked_auc: 0.8407 - val_loss: 0.4235 - val_masked_acc: 0.7653 - val_masked_auc: 0.8412\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4182 - masked_acc: 0.7656 - masked_auc: 0.8414 - val_loss: 0.4241 - val_masked_acc: 0.7660 - val_masked_auc: 0.8419\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4211 - masked_acc: 0.7662 - masked_auc: 0.8421 - val_loss: 0.4272 - val_masked_acc: 0.7666 - val_masked_auc: 0.8426\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 0.4277 - masked_acc: 0.7667 - masked_auc: 0.8427 - val_loss: 0.4207 - val_masked_acc: 0.7672 - val_masked_auc: 0.8432\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4205 - masked_acc: 0.7673 - masked_auc: 0.8434 - val_loss: 0.4217 - val_masked_acc: 0.7678 - val_masked_auc: 0.8439\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4225 - masked_acc: 0.7679 - masked_auc: 0.8440 - val_loss: 0.4214 - val_masked_acc: 0.7683 - val_masked_auc: 0.8445\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4194 - masked_acc: 0.7685 - masked_auc: 0.8447 - val_loss: 0.4243 - val_masked_acc: 0.7688 - val_masked_auc: 0.8451\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 0.4185 - masked_acc: 0.7690 - masked_auc: 0.8453 - val_loss: 0.4232 - val_masked_acc: 0.7694 - val_masked_auc: 0.8457\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4118 - masked_acc: 0.7696 - masked_auc: 0.8460 - val_loss: 0.4222 - val_masked_acc: 0.7699 - val_masked_auc: 0.8464\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4153 - masked_acc: 0.7701 - masked_auc: 0.8466 - val_loss: 0.4263 - val_masked_acc: 0.7704 - val_masked_auc: 0.8469\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 0.4131 - masked_acc: 0.7706 - masked_auc: 0.8471 - val_loss: 0.4290 - val_masked_acc: 0.7708 - val_masked_auc: 0.8474\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4216 - masked_acc: 0.7710 - masked_auc: 0.8476 - val_loss: 0.4314 - val_masked_acc: 0.7712 - val_masked_auc: 0.8478\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4160 - masked_acc: 0.7713 - masked_auc: 0.8480 - val_loss: 0.4309 - val_masked_acc: 0.7716 - val_masked_auc: 0.8482\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.4157 - masked_acc: 0.7717 - masked_auc: 0.8484 - val_loss: 0.4299 - val_masked_acc: 0.7720 - val_masked_auc: 0.8487\n",
            "23/23 [==============================] - 1s 42ms/step - loss: 0.4094 - masked_acc: 0.7721 - masked_auc: 0.8489\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.4428 - masked_acc: 0.7724 - masked_auc: 0.8491\n",
            "Test:  [0.4427585303783417, 0.7723705768585205, 0.8491165041923523]\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 8s 236ms/step - loss: 0.6159 - masked_acc: 0.5587 - masked_auc: 0.5532 - val_loss: 0.5145 - val_masked_acc: 0.6407 - val_masked_auc: 0.6907\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.5095 - masked_acc: 0.6600 - masked_auc: 0.7173 - val_loss: 0.4761 - val_masked_acc: 0.6852 - val_masked_auc: 0.7499\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4722 - masked_acc: 0.6937 - masked_auc: 0.7602 - val_loss: 0.4670 - val_masked_acc: 0.7057 - val_masked_auc: 0.7755\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4705 - masked_acc: 0.7106 - masked_auc: 0.7813 - val_loss: 0.4739 - val_masked_acc: 0.7183 - val_masked_auc: 0.7899\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4903 - masked_acc: 0.7200 - masked_auc: 0.7912 - val_loss: 0.4613 - val_masked_acc: 0.7240 - val_masked_auc: 0.7954\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4598 - masked_acc: 0.7262 - masked_auc: 0.7979 - val_loss: 0.4445 - val_masked_acc: 0.7301 - val_masked_auc: 0.8024\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4465 - masked_acc: 0.7320 - masked_auc: 0.8045 - val_loss: 0.4454 - val_masked_acc: 0.7355 - val_masked_auc: 0.8086\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4409 - masked_acc: 0.7370 - masked_auc: 0.8103 - val_loss: 0.4361 - val_masked_acc: 0.7397 - val_masked_auc: 0.8134\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4394 - masked_acc: 0.7411 - masked_auc: 0.8148 - val_loss: 0.4345 - val_masked_acc: 0.7432 - val_masked_auc: 0.8172\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.4402 - masked_acc: 0.7442 - masked_auc: 0.8183 - val_loss: 0.4330 - val_masked_acc: 0.7459 - val_masked_auc: 0.8202\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4372 - masked_acc: 0.7468 - masked_auc: 0.8212 - val_loss: 0.4310 - val_masked_acc: 0.7484 - val_masked_auc: 0.8230\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4335 - masked_acc: 0.7492 - masked_auc: 0.8238 - val_loss: 0.4323 - val_masked_acc: 0.7507 - val_masked_auc: 0.8256\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4284 - masked_acc: 0.7514 - masked_auc: 0.8264 - val_loss: 0.4339 - val_masked_acc: 0.7526 - val_masked_auc: 0.8277\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 111ms/step - loss: 0.4275 - masked_acc: 0.7533 - masked_auc: 0.8285 - val_loss: 0.4326 - val_masked_acc: 0.7542 - val_masked_auc: 0.8295\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 109ms/step - loss: 0.4360 - masked_acc: 0.7547 - masked_auc: 0.8300 - val_loss: 0.4279 - val_masked_acc: 0.7557 - val_masked_auc: 0.8311\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 110ms/step - loss: 0.4317 - masked_acc: 0.7562 - masked_auc: 0.8316 - val_loss: 0.4286 - val_masked_acc: 0.7570 - val_masked_auc: 0.8326\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4354 - masked_acc: 0.7574 - masked_auc: 0.8330 - val_loss: 0.4310 - val_masked_acc: 0.7581 - val_masked_auc: 0.8337\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 112ms/step - loss: 0.4303 - masked_acc: 0.7585 - masked_auc: 0.8341 - val_loss: 0.4314 - val_masked_acc: 0.7591 - val_masked_auc: 0.8348\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4265 - masked_acc: 0.7595 - masked_auc: 0.8352 - val_loss: 0.4273 - val_masked_acc: 0.7602 - val_masked_auc: 0.8359\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4211 - masked_acc: 0.7605 - masked_auc: 0.8363 - val_loss: 0.4249 - val_masked_acc: 0.7612 - val_masked_auc: 0.8371\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4162 - masked_acc: 0.7617 - masked_auc: 0.8376 - val_loss: 0.4258 - val_masked_acc: 0.7622 - val_masked_auc: 0.8382\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4239 - masked_acc: 0.7625 - masked_auc: 0.8385 - val_loss: 0.4336 - val_masked_acc: 0.7631 - val_masked_auc: 0.8392\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4230 - masked_acc: 0.7634 - masked_auc: 0.8395 - val_loss: 0.4335 - val_masked_acc: 0.7639 - val_masked_auc: 0.8400\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4352 - masked_acc: 0.7641 - masked_auc: 0.8401 - val_loss: 0.4289 - val_masked_acc: 0.7646 - val_masked_auc: 0.8405\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4274 - masked_acc: 0.7648 - masked_auc: 0.8407 - val_loss: 0.4302 - val_masked_acc: 0.7652 - val_masked_auc: 0.8411\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4178 - masked_acc: 0.7655 - masked_auc: 0.8414 - val_loss: 0.4269 - val_masked_acc: 0.7660 - val_masked_auc: 0.8419\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4191 - masked_acc: 0.7662 - masked_auc: 0.8422 - val_loss: 0.4276 - val_masked_acc: 0.7666 - val_masked_auc: 0.8427\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4122 - masked_acc: 0.7669 - masked_auc: 0.8429 - val_loss: 0.4250 - val_masked_acc: 0.7673 - val_masked_auc: 0.8434\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 0.4132 - masked_acc: 0.7675 - masked_auc: 0.8437 - val_loss: 0.4243 - val_masked_acc: 0.7679 - val_masked_auc: 0.8441\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 1s 109ms/step - loss: 0.4089 - masked_acc: 0.7682 - masked_auc: 0.8444 - val_loss: 0.4239 - val_masked_acc: 0.7685 - val_masked_auc: 0.8448\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4136 - masked_acc: 0.7687 - masked_auc: 0.8451 - val_loss: 0.4278 - val_masked_acc: 0.7691 - val_masked_auc: 0.8455\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4082 - masked_acc: 0.7693 - masked_auc: 0.8457 - val_loss: 0.4290 - val_masked_acc: 0.7697 - val_masked_auc: 0.8461\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.4105 - masked_acc: 0.7698 - masked_auc: 0.8463 - val_loss: 0.4252 - val_masked_acc: 0.7702 - val_masked_auc: 0.8468\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 1s 109ms/step - loss: 0.4128 - masked_acc: 0.7704 - masked_auc: 0.8470 - val_loss: 0.4240 - val_masked_acc: 0.7708 - val_masked_auc: 0.8474\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4034 - masked_acc: 0.7710 - masked_auc: 0.8476 - val_loss: 0.4247 - val_masked_acc: 0.7713 - val_masked_auc: 0.8480\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4062 - masked_acc: 0.7715 - masked_auc: 0.8482 - val_loss: 0.4276 - val_masked_acc: 0.7719 - val_masked_auc: 0.8486\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4078 - masked_acc: 0.7720 - masked_auc: 0.8488 - val_loss: 0.4278 - val_masked_acc: 0.7724 - val_masked_auc: 0.8492\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4019 - masked_acc: 0.7726 - masked_auc: 0.8494 - val_loss: 0.4341 - val_masked_acc: 0.7729 - val_masked_auc: 0.8498\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4032 - masked_acc: 0.7731 - masked_auc: 0.8500 - val_loss: 0.4262 - val_masked_acc: 0.7734 - val_masked_auc: 0.8503\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4040 - masked_acc: 0.7735 - masked_auc: 0.8505 - val_loss: 0.4320 - val_masked_acc: 0.7738 - val_masked_auc: 0.8507\n",
            "23/23 [==============================] - 1s 39ms/step - loss: 0.4058 - masked_acc: 0.7739 - masked_auc: 0.8510\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.4690 - masked_acc: 0.7741 - masked_auc: 0.8512\n",
            "Test:  [0.46902143955230713, 0.7741244435310364, 0.85117506980896]\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 8s 229ms/step - loss: 0.6090 - masked_acc: 0.5699 - masked_auc: 0.5630 - val_loss: 0.5278 - val_masked_acc: 0.6397 - val_masked_auc: 0.6863\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.5133 - masked_acc: 0.6578 - masked_auc: 0.7121 - val_loss: 0.4739 - val_masked_acc: 0.6838 - val_masked_auc: 0.7467\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 0.4778 - masked_acc: 0.6925 - masked_auc: 0.7575 - val_loss: 0.4538 - val_masked_acc: 0.7061 - val_masked_auc: 0.7745\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4596 - masked_acc: 0.7115 - masked_auc: 0.7808 - val_loss: 0.4360 - val_masked_acc: 0.7202 - val_masked_auc: 0.7910\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4548 - masked_acc: 0.7236 - masked_auc: 0.7950 - val_loss: 0.4386 - val_masked_acc: 0.7293 - val_masked_auc: 0.8013\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4539 - masked_acc: 0.7315 - masked_auc: 0.8037 - val_loss: 0.4267 - val_masked_acc: 0.7358 - val_masked_auc: 0.8087\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4476 - masked_acc: 0.7375 - masked_auc: 0.8104 - val_loss: 0.4250 - val_masked_acc: 0.7407 - val_masked_auc: 0.8140\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4393 - masked_acc: 0.7423 - masked_auc: 0.8157 - val_loss: 0.4236 - val_masked_acc: 0.7447 - val_masked_auc: 0.8184\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 1s 111ms/step - loss: 0.4355 - masked_acc: 0.7459 - masked_auc: 0.8197 - val_loss: 0.4210 - val_masked_acc: 0.7481 - val_masked_auc: 0.8220\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 1s 111ms/step - loss: 0.4406 - masked_acc: 0.7489 - masked_auc: 0.8229 - val_loss: 0.4264 - val_masked_acc: 0.7507 - val_masked_auc: 0.8248\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4511 - masked_acc: 0.7513 - masked_auc: 0.8254 - val_loss: 0.4265 - val_masked_acc: 0.7524 - val_masked_auc: 0.8266\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4438 - masked_acc: 0.7529 - masked_auc: 0.8272 - val_loss: 0.4247 - val_masked_acc: 0.7539 - val_masked_auc: 0.8282\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4423 - masked_acc: 0.7543 - masked_auc: 0.8288 - val_loss: 0.4227 - val_masked_acc: 0.7553 - val_masked_auc: 0.8298\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4312 - masked_acc: 0.7559 - masked_auc: 0.8304 - val_loss: 0.4192 - val_masked_acc: 0.7568 - val_masked_auc: 0.8314\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 107ms/step - loss: 0.4290 - masked_acc: 0.7573 - masked_auc: 0.8320 - val_loss: 0.4218 - val_masked_acc: 0.7581 - val_masked_auc: 0.8330\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4329 - masked_acc: 0.7585 - masked_auc: 0.8334 - val_loss: 0.4199 - val_masked_acc: 0.7593 - val_masked_auc: 0.8344\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4260 - masked_acc: 0.7597 - masked_auc: 0.8348 - val_loss: 0.4180 - val_masked_acc: 0.7604 - val_masked_auc: 0.8357\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.4250 - masked_acc: 0.7609 - masked_auc: 0.8361 - val_loss: 0.4181 - val_masked_acc: 0.7616 - val_masked_auc: 0.8369\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4316 - masked_acc: 0.7619 - masked_auc: 0.8372 - val_loss: 0.4189 - val_masked_acc: 0.7625 - val_masked_auc: 0.8379\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 1s 109ms/step - loss: 0.4218 - masked_acc: 0.7629 - masked_auc: 0.8383 - val_loss: 0.4175 - val_masked_acc: 0.7635 - val_masked_auc: 0.8391\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4207 - masked_acc: 0.7639 - masked_auc: 0.8394 - val_loss: 0.4222 - val_masked_acc: 0.7644 - val_masked_auc: 0.8401\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4252 - masked_acc: 0.7647 - masked_auc: 0.8404 - val_loss: 0.4247 - val_masked_acc: 0.7652 - val_masked_auc: 0.8410\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4210 - masked_acc: 0.7655 - masked_auc: 0.8413 - val_loss: 0.4200 - val_masked_acc: 0.7660 - val_masked_auc: 0.8418\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4190 - masked_acc: 0.7662 - masked_auc: 0.8421 - val_loss: 0.4205 - val_masked_acc: 0.7668 - val_masked_auc: 0.8427\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4182 - masked_acc: 0.7670 - masked_auc: 0.8430 - val_loss: 0.4209 - val_masked_acc: 0.7675 - val_masked_auc: 0.8434\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4248 - masked_acc: 0.7677 - masked_auc: 0.8437 - val_loss: 0.4139 - val_masked_acc: 0.7680 - val_masked_auc: 0.8441\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4162 - masked_acc: 0.7683 - masked_auc: 0.8444 - val_loss: 0.4187 - val_masked_acc: 0.7687 - val_masked_auc: 0.8448\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.4163 - masked_acc: 0.7689 - masked_auc: 0.8450 - val_loss: 0.4179 - val_masked_acc: 0.7692 - val_masked_auc: 0.8455\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 1s 109ms/step - loss: 0.4147 - masked_acc: 0.7694 - masked_auc: 0.8457 - val_loss: 0.4158 - val_masked_acc: 0.7698 - val_masked_auc: 0.8461\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 1s 113ms/step - loss: 0.4110 - masked_acc: 0.7701 - masked_auc: 0.8464 - val_loss: 0.4196 - val_masked_acc: 0.7704 - val_masked_auc: 0.8468\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 1s 111ms/step - loss: 0.4093 - masked_acc: 0.7707 - masked_auc: 0.8471 - val_loss: 0.4188 - val_masked_acc: 0.7710 - val_masked_auc: 0.8475\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 0.4055 - masked_acc: 0.7712 - masked_auc: 0.8477 - val_loss: 0.4210 - val_masked_acc: 0.7716 - val_masked_auc: 0.8481\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4093 - masked_acc: 0.7718 - masked_auc: 0.8483 - val_loss: 0.4196 - val_masked_acc: 0.7720 - val_masked_auc: 0.8486\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 1s 111ms/step - loss: 0.4045 - masked_acc: 0.7723 - masked_auc: 0.8488 - val_loss: 0.4261 - val_masked_acc: 0.7725 - val_masked_auc: 0.8491\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 1s 109ms/step - loss: 0.4112 - masked_acc: 0.7727 - masked_auc: 0.8493 - val_loss: 0.4252 - val_masked_acc: 0.7730 - val_masked_auc: 0.8496\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 1s 110ms/step - loss: 0.4121 - masked_acc: 0.7731 - masked_auc: 0.8498 - val_loss: 0.4234 - val_masked_acc: 0.7734 - val_masked_auc: 0.8501\n",
            "23/23 [==============================] - 1s 44ms/step - loss: 0.4119 - masked_acc: 0.7736 - masked_auc: 0.8503\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.4835 - masked_acc: 0.7738 - masked_auc: 0.8505\n",
            "Test:  [0.48348137736320496, 0.773759663105011, 0.8504939079284668]\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 8s 237ms/step - loss: 0.6073 - masked_acc: 0.5390 - masked_auc: 0.5635 - val_loss: 0.4957 - val_masked_acc: 0.6378 - val_masked_auc: 0.6941\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4974 - masked_acc: 0.6589 - masked_auc: 0.7213 - val_loss: 0.4644 - val_masked_acc: 0.6875 - val_masked_auc: 0.7566\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4679 - masked_acc: 0.6966 - masked_auc: 0.7670 - val_loss: 0.4411 - val_masked_acc: 0.7108 - val_masked_auc: 0.7838\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 0.4602 - masked_acc: 0.7158 - masked_auc: 0.7890 - val_loss: 0.4707 - val_masked_acc: 0.7241 - val_masked_auc: 0.7981\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 0.4616 - masked_acc: 0.7268 - masked_auc: 0.8007 - val_loss: 0.4440 - val_masked_acc: 0.7318 - val_masked_auc: 0.8057\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4622 - masked_acc: 0.7334 - masked_auc: 0.8072 - val_loss: 0.4363 - val_masked_acc: 0.7369 - val_masked_auc: 0.8109\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 0.4477 - masked_acc: 0.7385 - masked_auc: 0.8125 - val_loss: 0.4328 - val_masked_acc: 0.7416 - val_masked_auc: 0.8158\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 1s 107ms/step - loss: 0.4376 - masked_acc: 0.7428 - masked_auc: 0.8172 - val_loss: 0.4251 - val_masked_acc: 0.7454 - val_masked_auc: 0.8201\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4379 - masked_acc: 0.7465 - masked_auc: 0.8212 - val_loss: 0.4219 - val_masked_acc: 0.7488 - val_masked_auc: 0.8237\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 1s 110ms/step - loss: 0.4355 - masked_acc: 0.7497 - masked_auc: 0.8246 - val_loss: 0.4353 - val_masked_acc: 0.7515 - val_masked_auc: 0.8266\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 1s 112ms/step - loss: 0.4448 - masked_acc: 0.7523 - masked_auc: 0.8270 - val_loss: 0.4267 - val_masked_acc: 0.7537 - val_masked_auc: 0.8283\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 1s 111ms/step - loss: 0.4371 - masked_acc: 0.7543 - masked_auc: 0.8288 - val_loss: 0.4285 - val_masked_acc: 0.7555 - val_masked_auc: 0.8302\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4337 - masked_acc: 0.7560 - masked_auc: 0.8306 - val_loss: 0.4227 - val_masked_acc: 0.7572 - val_masked_auc: 0.8319\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4323 - masked_acc: 0.7576 - masked_auc: 0.8324 - val_loss: 0.4224 - val_masked_acc: 0.7586 - val_masked_auc: 0.8335\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 0.4335 - masked_acc: 0.7590 - masked_auc: 0.8338 - val_loss: 0.4235 - val_masked_acc: 0.7598 - val_masked_auc: 0.8348\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 0.4308 - masked_acc: 0.7602 - masked_auc: 0.8352 - val_loss: 0.4217 - val_masked_acc: 0.7611 - val_masked_auc: 0.8362\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4317 - masked_acc: 0.7614 - masked_auc: 0.8366 - val_loss: 0.4233 - val_masked_acc: 0.7622 - val_masked_auc: 0.8375\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 107ms/step - loss: 0.4295 - masked_acc: 0.7626 - masked_auc: 0.8378 - val_loss: 0.4296 - val_masked_acc: 0.7632 - val_masked_auc: 0.8386\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 0.4290 - masked_acc: 0.7635 - masked_auc: 0.8388 - val_loss: 0.4255 - val_masked_acc: 0.7641 - val_masked_auc: 0.8395\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 1s 111ms/step - loss: 0.4295 - masked_acc: 0.7644 - masked_auc: 0.8397 - val_loss: 0.4240 - val_masked_acc: 0.7649 - val_masked_auc: 0.8403\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 0.4281 - masked_acc: 0.7652 - masked_auc: 0.8406 - val_loss: 0.4241 - val_masked_acc: 0.7656 - val_masked_auc: 0.8411\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4227 - masked_acc: 0.7659 - masked_auc: 0.8413 - val_loss: 0.4233 - val_masked_acc: 0.7664 - val_masked_auc: 0.8419\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 110ms/step - loss: 0.4268 - masked_acc: 0.7666 - masked_auc: 0.8421 - val_loss: 0.4276 - val_masked_acc: 0.7671 - val_masked_auc: 0.8426\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.4249 - masked_acc: 0.7673 - masked_auc: 0.8428 - val_loss: 0.4226 - val_masked_acc: 0.7678 - val_masked_auc: 0.8433\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4193 - masked_acc: 0.7680 - masked_auc: 0.8436 - val_loss: 0.4205 - val_masked_acc: 0.7685 - val_masked_auc: 0.8441\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4177 - masked_acc: 0.7686 - masked_auc: 0.8443 - val_loss: 0.4229 - val_masked_acc: 0.7691 - val_masked_auc: 0.8448\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4146 - masked_acc: 0.7693 - masked_auc: 0.8450 - val_loss: 0.4301 - val_masked_acc: 0.7697 - val_masked_auc: 0.8454\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4221 - masked_acc: 0.7698 - masked_auc: 0.8456 - val_loss: 0.4219 - val_masked_acc: 0.7702 - val_masked_auc: 0.8460\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4198 - masked_acc: 0.7704 - masked_auc: 0.8462 - val_loss: 0.4209 - val_masked_acc: 0.7707 - val_masked_auc: 0.8466\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4165 - masked_acc: 0.7709 - masked_auc: 0.8468 - val_loss: 0.4300 - val_masked_acc: 0.7712 - val_masked_auc: 0.8472\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4330 - masked_acc: 0.7713 - masked_auc: 0.8473 - val_loss: 0.4385 - val_masked_acc: 0.7715 - val_masked_auc: 0.8474\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4322 - masked_acc: 0.7715 - masked_auc: 0.8474 - val_loss: 0.4273 - val_masked_acc: 0.7717 - val_masked_auc: 0.8476\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4220 - masked_acc: 0.7718 - masked_auc: 0.8477 - val_loss: 0.4262 - val_masked_acc: 0.7720 - val_masked_auc: 0.8480\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4187 - masked_acc: 0.7721 - masked_auc: 0.8481 - val_loss: 0.4232 - val_masked_acc: 0.7724 - val_masked_auc: 0.8484\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 0.4041 - masked_acc: 0.7726 - masked_auc: 0.8486 - val_loss: 0.4224 - val_masked_acc: 0.7728 - val_masked_auc: 0.8489\n",
            "23/23 [==============================] - 1s 43ms/step - loss: 0.4080 - masked_acc: 0.7730 - masked_auc: 0.8491\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.4482 - masked_acc: 0.7732 - masked_auc: 0.8494\n",
            "Test:  [0.44816094636917114, 0.7732155919075012, 0.8494004607200623]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVmumHMz3lx",
        "outputId": "77c3f007-5331-4c54-e7e9-bae5518b16f8"
      },
      "source": [
        "t_eval = np.array(test_eval)\n",
        "print(\"test avg loss: \", np.mean(t_eval[:, 0]), \"+/-\" ,np.std(t_eval[:, 0]))\n",
        "print(\"test avg acc: \", np.mean(t_eval[:, 1]),  \"+/-\" ,np.std(t_eval[:, 1]))\n",
        "print(\"test avg auc: \", np.mean(t_eval[:, 2]), \"+/-\" ,np.std(t_eval[:, 2]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test avg loss:  0.45977904796600344 +/- 0.014767823383679051\n",
            "test avg acc:  0.7733503341674804 +/- 0.0005915450778629877\n",
            "test avg auc:  0.8502392530441284 +/- 0.0008365749087196946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9MM_CXWz5K6",
        "outputId": "88d409da-1822-4795-c8a2-a1635e936e26"
      },
      "source": [
        "t_eval = np.array(train_eval)\n",
        "print(\"train avg loss: \", np.mean(t_eval[:, 0]), \"+/-\" ,np.std(t_eval[:, 0]))\n",
        "print(\"train avg acc: \", np.mean(t_eval[:, 1]),  \"+/-\" ,np.std(t_eval[:, 1]))\n",
        "print(\"train avg auc: \", np.mean(t_eval[:, 2]), \"+/-\" ,np.std(t_eval[:, 2]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train avg loss:  0.40575259923934937 +/- 0.0063762673236999684\n",
            "train avg acc:  0.7730757355690002 +/- 0.0006301884449222985\n",
            "train avg auc:  0.8499410152435303 +/- 0.0007978243477947796\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}