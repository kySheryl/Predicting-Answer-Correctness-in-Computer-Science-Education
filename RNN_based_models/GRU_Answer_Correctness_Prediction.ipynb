{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU_Answer_Correctness_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "farifxiKU1aB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import random\n",
        "from random import choice\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, GRU, Concatenate, Embedding, Flatten, Activation, Dropout\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.python.client import device_lib\n",
        "warnings.filterwarnings('ignore')\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOson70GPOXb"
      },
      "source": [
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQsVybBH3VPw"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwtTusGxrmAE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "1430f3cf-038b-485e-b5c4-c1686b1f34fd"
      },
      "source": [
        "# load the data\n",
        "import pandas as pd\n",
        "dataframe = pd.read_csv(\"/content/drive/MyDrive/dlresearch/Practice_Log_Demographics.csv\") \n",
        "dataframe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>term</th>\n",
              "      <th>Final_Exam</th>\n",
              "      <th>Final_Exam_Std</th>\n",
              "      <th>Final_Exam_Percent</th>\n",
              "      <th>Final_Exam_0_1</th>\n",
              "      <th>Final_Exam_log_trans</th>\n",
              "      <th>Dropped</th>\n",
              "      <th>SNPSHT_RPT_DT</th>\n",
              "      <th>STDNT_SEX_CD</th>\n",
              "      <th>gender</th>\n",
              "      <th>birthYear</th>\n",
              "      <th>birthMonth</th>\n",
              "      <th>STDNT_ASIAN_IND</th>\n",
              "      <th>STDNT_BLACK_IND</th>\n",
              "      <th>STDNT_HWIAN_IND</th>\n",
              "      <th>STDNT_HSPNC_IND</th>\n",
              "      <th>STDNT_NTV_AMRCN_IND</th>\n",
              "      <th>STDNT_WHITE_IND</th>\n",
              "      <th>STDNT_ETHNC_GRP_CD</th>\n",
              "      <th>ethnicity</th>\n",
              "      <th>STDNT_MULTI_ETHNC_IND</th>\n",
              "      <th>STDNT_HSPNC_LATINO_IND</th>\n",
              "      <th>nativeEnglish</th>\n",
              "      <th>FIRST_US_PRMNNT_RES_PSTL_CD</th>\n",
              "      <th>FIRST_US_PRMNNT_RES_PSTL_5_CD</th>\n",
              "      <th>FRST_FRGN_PRMNNT_RES_CNTRY_CD</th>\n",
              "      <th>permanentCountry</th>\n",
              "      <th>STDNT_CTZN_STAT_CD</th>\n",
              "      <th>USCitizenship</th>\n",
              "      <th>STDNT_CTZN_CNTRY_1_CD</th>\n",
              "      <th>citizenship</th>\n",
              "      <th>STDNT_CTZN_CNTRY_2_CD</th>\n",
              "      <th>STDNT_CTZN_CNTRY_2_DES</th>\n",
              "      <th>international</th>\n",
              "      <th>FIRST_TERM_ATTND_CD</th>\n",
              "      <th>firstTerm</th>\n",
              "      <th>FIRST_TERM_ATTND_BEGIN_YR_MO</th>\n",
              "      <th>FIRST_TERM_ATTND_END_YR_MO</th>\n",
              "      <th>LAST_TERM_ATTND_CD</th>\n",
              "      <th>...</th>\n",
              "      <th>Freshman</th>\n",
              "      <th>Junior</th>\n",
              "      <th>Sophomore</th>\n",
              "      <th>Senior</th>\n",
              "      <th>Minors_1</th>\n",
              "      <th>Minors_2OrMore</th>\n",
              "      <th>athlete_1</th>\n",
              "      <th>honorsPro_1</th>\n",
              "      <th>program_Cat</th>\n",
              "      <th>LSA</th>\n",
              "      <th>programBusiness</th>\n",
              "      <th>programEngineering</th>\n",
              "      <th>programInformation</th>\n",
              "      <th>programOther</th>\n",
              "      <th>interaction_count</th>\n",
              "      <th>activeCode_count</th>\n",
              "      <th>codelens_count</th>\n",
              "      <th>mChoice_count</th>\n",
              "      <th>pageVideoViews_count</th>\n",
              "      <th>parsons_count</th>\n",
              "      <th>interaction_days</th>\n",
              "      <th>spacing</th>\n",
              "      <th>user_id.x</th>\n",
              "      <th>practice_count</th>\n",
              "      <th>practice_days</th>\n",
              "      <th>user_id.y</th>\n",
              "      <th>course_name.y</th>\n",
              "      <th>chapter_label</th>\n",
              "      <th>sub_chapter_label</th>\n",
              "      <th>question_name</th>\n",
              "      <th>i_interval</th>\n",
              "      <th>e_factor</th>\n",
              "      <th>q</th>\n",
              "      <th>trials_num</th>\n",
              "      <th>day's_available_flashcards</th>\n",
              "      <th>start_practice</th>\n",
              "      <th>end_practice</th>\n",
              "      <th>timezoneoffset</th>\n",
              "      <th>next_eligible_date</th>\n",
              "      <th>days_offset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>WN 2018</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.199159</td>\n",
              "      <td>89.2</td>\n",
              "      <td>0.883168</td>\n",
              "      <td>2.4681</td>\n",
              "      <td>0</td>\n",
              "      <td>09-FEB-21</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>1998</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Native</td>\n",
              "      <td>02052-3110</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Citzn</td>\n",
              "      <td>USA</td>\n",
              "      <td>United States</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>FA 2016</td>\n",
              "      <td>2016/09</td>\n",
              "      <td>2016/12</td>\n",
              "      <td>2270</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LS&amp;A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5913.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>636.0</td>\n",
              "      <td>1342.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>23.016143</td>\n",
              "      <td>148</td>\n",
              "      <td>493</td>\n",
              "      <td>44</td>\n",
              "      <td>148</td>\n",
              "      <td>UMSI106</td>\n",
              "      <td>Functions</td>\n",
              "      <td>DecodingaFunction</td>\n",
              "      <td>test_questionfunctions_3_3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "      <td>2018-02-10 12:03:08</td>\n",
              "      <td>2018-02-10 12:04:01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>WN 2018</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.199159</td>\n",
              "      <td>89.2</td>\n",
              "      <td>0.883168</td>\n",
              "      <td>2.4681</td>\n",
              "      <td>0</td>\n",
              "      <td>09-FEB-21</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>1998</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Native</td>\n",
              "      <td>02052-3110</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Citzn</td>\n",
              "      <td>USA</td>\n",
              "      <td>United States</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>FA 2016</td>\n",
              "      <td>2016/09</td>\n",
              "      <td>2016/12</td>\n",
              "      <td>2270</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LS&amp;A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5913.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>636.0</td>\n",
              "      <td>1342.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>23.016143</td>\n",
              "      <td>148</td>\n",
              "      <td>493</td>\n",
              "      <td>44</td>\n",
              "      <td>148</td>\n",
              "      <td>UMSI106</td>\n",
              "      <td>Tuples</td>\n",
              "      <td>UnpackingDictionaryItems</td>\n",
              "      <td>ee_ch09_05</td>\n",
              "      <td>1</td>\n",
              "      <td>1.40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-02-20 13:31:37</td>\n",
              "      <td>2018-02-20 13:31:41</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>WN 2018</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.199159</td>\n",
              "      <td>89.2</td>\n",
              "      <td>0.883168</td>\n",
              "      <td>2.4681</td>\n",
              "      <td>0</td>\n",
              "      <td>09-FEB-21</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>1998</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Native</td>\n",
              "      <td>02052-3110</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Citzn</td>\n",
              "      <td>USA</td>\n",
              "      <td>United States</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>FA 2016</td>\n",
              "      <td>2016/09</td>\n",
              "      <td>2016/12</td>\n",
              "      <td>2270</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LS&amp;A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5913.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>636.0</td>\n",
              "      <td>1342.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>23.016143</td>\n",
              "      <td>148</td>\n",
              "      <td>493</td>\n",
              "      <td>44</td>\n",
              "      <td>148</td>\n",
              "      <td>UMSI106</td>\n",
              "      <td>NestedData</td>\n",
              "      <td>DebuggingNestedData</td>\n",
              "      <td>ee_nested_data_011</td>\n",
              "      <td>15</td>\n",
              "      <td>2.50</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2018-03-16 14:09:57</td>\n",
              "      <td>2018-03-16 14:10:33</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>WN 2018</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.199159</td>\n",
              "      <td>89.2</td>\n",
              "      <td>0.883168</td>\n",
              "      <td>2.4681</td>\n",
              "      <td>0</td>\n",
              "      <td>09-FEB-21</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>1998</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Native</td>\n",
              "      <td>02052-3110</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Citzn</td>\n",
              "      <td>USA</td>\n",
              "      <td>United States</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>FA 2016</td>\n",
              "      <td>2016/09</td>\n",
              "      <td>2016/12</td>\n",
              "      <td>2270</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LS&amp;A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5913.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>636.0</td>\n",
              "      <td>1342.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>23.016143</td>\n",
              "      <td>148</td>\n",
              "      <td>493</td>\n",
              "      <td>44</td>\n",
              "      <td>148</td>\n",
              "      <td>UMSI106</td>\n",
              "      <td>SimplePythonData</td>\n",
              "      <td>FunctionCalls</td>\n",
              "      <td>exercise_functionCalls_1</td>\n",
              "      <td>16</td>\n",
              "      <td>2.60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>2018-01-30 14:15:49</td>\n",
              "      <td>2018-01-30 14:15:58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>WN 2018</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.199159</td>\n",
              "      <td>89.2</td>\n",
              "      <td>0.883168</td>\n",
              "      <td>2.4681</td>\n",
              "      <td>0</td>\n",
              "      <td>09-FEB-21</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>1998</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>White</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Native</td>\n",
              "      <td>02052-3110</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>U.S. Citzn</td>\n",
              "      <td>USA</td>\n",
              "      <td>United States</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2110</td>\n",
              "      <td>FA 2016</td>\n",
              "      <td>2016/09</td>\n",
              "      <td>2016/12</td>\n",
              "      <td>2270</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LS&amp;A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5913.0</td>\n",
              "      <td>2965.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>636.0</td>\n",
              "      <td>1342.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>23.016143</td>\n",
              "      <td>148</td>\n",
              "      <td>493</td>\n",
              "      <td>44</td>\n",
              "      <td>148</td>\n",
              "      <td>UMSI106</td>\n",
              "      <td>SimplePythonData</td>\n",
              "      <td>DataTypes</td>\n",
              "      <td>test_question2_1_2</td>\n",
              "      <td>59</td>\n",
              "      <td>2.10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>2018-03-13 14:22:54</td>\n",
              "      <td>2018-03-13 14:23:16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 760 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0     term  ...  next_eligible_date  days_offset\n",
              "0           1  WN 2018  ...                   0           23\n",
              "1           2  WN 2018  ...                   0           33\n",
              "2           3  WN 2018  ...                   0           57\n",
              "3           4  WN 2018  ...                   0           12\n",
              "4           5  WN 2018  ...                   0           54\n",
              "\n",
              "[5 rows x 760 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCbHnEp_Vir0"
      },
      "source": [
        "#sort data based on timestamp\n",
        "dataframe = dataframe.sort_values(by=['start_practice'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RvF7BvEr98o"
      },
      "source": [
        "FEATURES = ['Repeated', 'chapter_label', 'sub_chapter_label','question_name','user_id','term',\n",
        "            'STDNT_SEX_CD', \n",
        "            'NonNativeEnglish',\n",
        "            'White',\n",
        "            'Asian',\n",
        "            'WhiteOrAsian',\n",
        "            'Hispanic',\n",
        "            'AfricanAmerican',\n",
        "            'OtherEthnicities',\n",
        "            'NonWhiteOrAsian',\n",
        "            'STDNT_CTZN_STAT_CD', 'international', \n",
        "            'gradingType',\n",
        "            'birthYear',\n",
        "            'exclClassCumGPA',\n",
        "            'Freshman',\n",
        "            'Junior',\n",
        "            'Sophomore',\n",
        "            'Senior',\n",
        "            'termCreditsGPA',\n",
        "            'termCreditsNoGPA',\n",
        "            'athlete_1',\n",
        "            'honorsPro',\n",
        "            'LSA', 'programBusiness', 'programEngineering', \n",
        "            'programInformation', 'programOther',\n",
        "            'HSCalculusTaken', \n",
        "            'highSchoolGPA', \n",
        "            'majorsCount', 'minorsCount',\n",
        "            'PREV_TERM_CUM_GPA',\n",
        "            'classGraded', 'classHonors', \n",
        "            'Pass_Fail', \n",
        "            'parentsGraduateEdu',  'minorityGroup', \n",
        "            'q',\n",
        "            'available_flashcards', \n",
        "            'start_practice', \n",
        "            'end_practice',\n",
        "            'days_offset']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn341X7lSztl"
      },
      "source": [
        "dataframe['available_flashcards'] = dataframe[\"day's_available_flashcards\"][:]\n",
        "dataframe = dataframe.drop([\"day's_available_flashcards\"], axis=1)\n",
        "dataframe['user_id'] = dataframe[\"user_id.y\"][:]\n",
        "dataframe = dataframe.drop([\"user_id.y\"], axis=1)\n",
        "dataframe = dataframe.drop([\"user_id.x\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_BRggOjbBjN",
        "outputId": "42bff7db-08dd-48dc-a73a-d048508053b7"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXM7MlonVeRL",
        "outputId": "d29ccec0-522f-4e3c-e0b2-39b2e1eaef74"
      },
      "source": [
        "dataframe = dataframe[FEATURES]\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WAFv5z3A6fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27abf84-ec3c-4b88-e1fb-927d94b41898"
      },
      "source": [
        "dataframe = dataframe.fillna(0)\n",
        "#transform q value so that if it is >= 4 it would be\n",
        "#considered as 1 (correct) and if <4 0(incorrect)\n",
        "dataframe['answer_correct'] = np.where(dataframe['q']>=4, 1, 0)\n",
        "dataframe['answer_correct'].mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5326395552433993"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRKqF_0IXZgj"
      },
      "source": [
        "#label encode term, chapter_label, question_name, sub_chapter_label columns\n",
        "dataframe['term'] = dataframe['term'].astype('category')\n",
        "dataframe['user_id'] = dataframe['user_id'].astype(int)\n",
        "dataframe['user_id'] = dataframe['user_id'].astype(str)\n",
        "dataframe['user_id'] = dataframe['term'].str.cat(dataframe['user_id'], sep=':')\n",
        "dataframe['user_id'] = dataframe['user_id'].astype('category')\n",
        "dataframe['chapter_label'] = dataframe['chapter_label'].astype('category')\n",
        "dataframe['sub_chapter_label'] = dataframe['sub_chapter_label'].astype('category')\n",
        "dataframe['question_name'] = dataframe['question_name'].astype('category')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVl_067GWkDe"
      },
      "source": [
        "#calculate time_lag and prev_time_elapsed\n",
        "dataframe['prev_time_elapsed'] = None\n",
        "dataframe['time_lag'] = None\n",
        "dataframe['time_lag'] = dataframe['time_lag'].astype(np.float)\n",
        "dataframe['prev_time_elapsed'] = dataframe['prev_time_elapsed'].astype(np.float)\n",
        "dataframe.start_practice = pd.to_datetime(dataframe.start_practice, format='%Y-%m-%d %H:%M:%S')\n",
        "dataframe.end_practice = pd.to_datetime(dataframe.end_practice, format='%Y-%m-%d %H:%M:%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xrkcn9VhWETm",
        "outputId": "fd73f3de-5f56-42a8-9539-3f3b79a5a6e2"
      },
      "source": [
        "from tqdm import tqdm\n",
        "for user in tqdm(dataframe['user_id'].unique()):\n",
        "    tmp_user = dataframe[dataframe['user_id']==user]\n",
        "    tmp_time_elapsed = tmp_user.end_practice.apply(lambda a: a.timestamp()) - tmp_user.start_practice.apply(lambda a: a.timestamp())\n",
        "    #shifting time elapsed by one\n",
        "    #so that time_elapsed row for each question\n",
        "    #would refer to the time that user took to answer\n",
        "    #previous question\n",
        "    tmp_time_elapsed = np.insert(np.array(tmp_time_elapsed[:-1]), 0, -1., axis=0)\n",
        "    indices = tmp_user.index\n",
        "    start_row = indices[0]\n",
        "    dataframe['time_lag'].iloc[start_row] = -1\n",
        "    time_substrahend = tmp_user.start_practice.iloc[:-1]\n",
        "    time_substrahend = time_substrahend.apply(lambda a: a.timestamp())\n",
        "    time_substrahend = np.array(time_substrahend)\n",
        "\n",
        "    time_minuend = tmp_user.start_practice.iloc[1:]\n",
        "    time_minuend = time_minuend.apply(lambda a: a.timestamp())\n",
        "    time_minuend = np.array(time_minuend)\n",
        "    \n",
        "    dataframe['prev_time_elapsed'].iloc[indices] = tmp_time_elapsed\n",
        "    dataframe['time_lag'].iloc[indices[1:]] = time_minuend - time_substrahend"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 887/887 [00:11<00:00, 78.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S96YQhUGZ-Au",
        "outputId": "9811006c-0088-4b03-dc79-6ce89dc611bc"
      },
      "source": [
        "dataframe  = dataframe[dataframe['Repeated'] == ' ']\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t8yZ0XvWwzz"
      },
      "source": [
        "#drop column end_practice\n",
        "dataframe.drop(columns=['end_practice'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E42raRIrYYXz"
      },
      "source": [
        "# calculate the age feature\n",
        "dataframe['term_value'] = [int(ele[3:]) for ele in dataframe['term']]\n",
        "dataframe['age'] = dataframe['term_value'] - dataframe['birthYear']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcMVQXueYnMh"
      },
      "source": [
        "# drop term_value and birthYear column\n",
        "dataframe.drop(columns=['term_value', 'birthYear'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjRF5szkcQE0"
      },
      "source": [
        "# convert minors_count to int value\n",
        "new_minors_count = []\n",
        "for i in dataframe['minorsCount']:\n",
        "  if i == 0 or i == '0':\n",
        "    new_minors_count.append(0)\n",
        "  elif i == '1 Minor':\n",
        "    new_minors_count.append(1)\n",
        "  else:\n",
        "    new_minors_count.append(2)\n",
        "\n",
        "dataframe['minorsCount'] = new_minors_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VdzvJSw0KUU",
        "outputId": "6393784f-234e-4935-9cee-cbcf56b5630a"
      },
      "source": [
        "print(\"we have \", dataframe['user_id'].nunique(),\" users in total.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we have  881  users in total.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUaGk9L9g13t"
      },
      "source": [
        "for category in ['term','chapter_label', 'sub_chapter_label', 'question_name']:\n",
        "  dataframe[category] =  dataframe[category].cat.codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkyvcYvlhliG"
      },
      "source": [
        "NUMERIC_FEATURE =  ['age',\n",
        "            'exclClassCumGPA',\n",
        "            'termCreditsGPA',\n",
        "            'termCreditsNoGPA',\n",
        "            'highSchoolGPA', \n",
        "            'majorsCount', 'minorsCount',\n",
        "            'PREV_TERM_CUM_GPA',\n",
        "            'available_flashcards', \n",
        "            'days_offset', \n",
        "            'prev_time_elapsed',\n",
        "             'time_lag']\n",
        "# z-score normalize the numerical features\n",
        "for f in NUMERIC_FEATURE:\n",
        "  m = dataframe[f].mean()\n",
        "  std = dataframe[f].std()\n",
        "  dataframe[f] = (dataframe[f] - m)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo2OH-d5kWp1"
      },
      "source": [
        "FEATURE_TRANS =  ['answer_correct', 'chapter_label', 'sub_chapter_label','question_name','user_id','term',\n",
        "                  'STDNT_SEX_CD', \n",
        "                    'White','Asian','NonWhiteOrAsian',\n",
        "            'STDNT_CTZN_STAT_CD', 'international', \n",
        "            'age',\n",
        "            'exclClassCumGPA',\n",
        "           'Freshman',\n",
        "            'Junior',\n",
        "            'Sophomore',\n",
        "            'Senior',\n",
        "            'termCreditsGPA',\n",
        "            'termCreditsNoGPA',\n",
        "            'athlete_1',\n",
        "            'honorsPro',\n",
        "            'LSA', 'programBusiness', 'programEngineering', \n",
        "            'programInformation', 'programOther',\n",
        "            'HSCalculusTaken', \n",
        "            'highSchoolGPA', \n",
        "            'majorsCount', 'minorsCount',\n",
        "            'PREV_TERM_CUM_GPA',\n",
        "            'classGraded', 'classHonors', \n",
        "            'Pass_Fail', \n",
        "            'parentsGraduateEdu',  'minorityGroup', \n",
        "            'available_flashcards', \n",
        "            'days_offset', 'prev_time_elapsed',\n",
        "             'time_lag']\n",
        "grouped_data = dataframe[FEATURE_TRANS].groupby(['user_id']).apply(lambda r: (\n",
        "                r['answer_correct'],\n",
        "                r['term'],\n",
        "                r['chapter_label'],\n",
        "                r['sub_chapter_label'],\n",
        "                r['question_name'],\n",
        "                np.array([r['STDNT_SEX_CD'],r['STDNT_CTZN_STAT_CD'], r['international'], \n",
        "                  r['White'],r['Asian'],r['NonWhiteOrAsian'],\n",
        "                 r['age'],r['exclClassCumGPA'],\n",
        "                r['Freshman'], r['Junior'], r['Sophomore'], r['Senior'],\n",
        "                r['termCreditsGPA'], r['termCreditsNoGPA'],\n",
        "                r['athlete_1'], r['honorsPro'],\n",
        "                r['LSA'], r['programBusiness'], r['programEngineering'], \n",
        "                r['programInformation'], r['programOther'],\n",
        "                r['HSCalculusTaken'],  r['highSchoolGPA'], \n",
        "                r['majorsCount'], r['minorsCount'],\n",
        "                r['PREV_TERM_CUM_GPA'], \n",
        "                r['parentsGraduateEdu'], r['minorityGroup'],\n",
        "                r['available_flashcards'],\n",
        "                r['days_offset'],\n",
        "                r['prev_time_elapsed'],\n",
        "                r['time_lag']\n",
        "              ]).transpose()\n",
        "                ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXLqQnjrlp_4",
        "outputId": "c801da79-f0bf-4fe5-b19c-7c799fe01556"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYGQasdTrmP0"
      },
      "source": [
        "# remove students who don't have make any interactions with the tool\n",
        "toRemove = []\n",
        "for index in grouped_data.index:\n",
        "  if len(grouped_data[index][0]) <= 10:\n",
        "    toRemove.append(index)\n",
        "grouped_data = grouped_data.drop(index=toRemove)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kZqV9siDyNb"
      },
      "source": [
        "#SETTINGS -> can be modified at any time\n",
        "MAXLENGTH = 500\n",
        "EMBEDDING_DIM = 128\n",
        "DENSE_NEURON = 16\n",
        "GRU_NEURON = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MksD1JizpPn"
      },
      "source": [
        "FEATURES_SIZE = 38\n",
        "CHAPTER_SIZE = 38\n",
        "SUB_CHAPTER_SIZE = 222\n",
        "QUESTION_SIZE = 1065"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY3Thp6d0NaT"
      },
      "source": [
        "#create dataset class\n",
        "#to prepare it for train, valid, and test sets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class SPACE_DATASET(Dataset):\n",
        "    def __init__(self, data, maxlength = 100):\n",
        "        super(SPACE_DATASET, self).__init__()\n",
        "        self.maxlength = maxlength\n",
        "        self.data = data\n",
        "        self.users = list()\n",
        "        for user in data.index:\n",
        "            self.users.append(user)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "    \n",
        "    def __getitem__(self, ix):\n",
        "        user = self.users[ix]\n",
        "        user = user\n",
        "        target, term, ch_label, sub_ch_label, ques_name, features = self.data[user]\n",
        "        \n",
        "        #0s should be used as padding values\n",
        "        ori_target = target.values \n",
        "        term = term.values\n",
        "        ch_label = ch_label.values + 1\n",
        "        sub_ch_label = sub_ch_label.values +1\n",
        "        ques_name = ques_name.values + 1\n",
        "        \n",
        "        n = len(ch_label)\n",
        "\n",
        "        # one hot for term\n",
        "        term_encode = [0]*6\n",
        "        term_encode[term[0]] = 1\n",
        "        shifted_target= []\n",
        "\n",
        "        \n",
        "        # get  user interaction informations in the previous MAXLEN interactions\n",
        "        if n > self.maxlength:\n",
        "          ch_label = ch_label[-self.maxlength:]\n",
        "          sub_ch_label = sub_ch_label[-self.maxlength:]\n",
        "          ques_name = ques_name[-self.maxlength:]\n",
        "          features = features[-self.maxlength:]\n",
        "          target = ori_target[-self.maxlength:]\n",
        "          shifted_target = ori_target[ (-self.maxlength - 1) :-1]\n",
        "        else:\n",
        "          ch_label = [0]*(self.maxlength - n)+list(ch_label[:])\n",
        "          sub_ch_label = [0]*(self.maxlength - n)+list(sub_ch_label[:])\n",
        "          ques_name = [0]*(self.maxlength - n)+list(ques_name[:])\n",
        "          features = [[0]*len(features[0])]*(self.maxlength  - n)+list(features[:])\n",
        "          target = [-1]*(self.maxlength - n) + list(ori_target[:])\n",
        "          shifted_target = [-1]*(self.maxlength + 1 - n) + list(ori_target[:-1])\n",
        "\n",
        "        new_features = []\n",
        "        count = 0\n",
        "        for f in features:\n",
        "          temp = list(f)\n",
        "          temp.extend(term_encode)\n",
        "          # temp.append(shifted_target[count]) #uncomment this line for include previous response feature\n",
        "          new_features.append(temp)\n",
        "          count += 1\n",
        "        features = new_features\n",
        "        return ch_label,sub_ch_label,ques_name,features,target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xc90-aLzxat"
      },
      "source": [
        "## KFOLD - GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzJrljnjzypP",
        "outputId": "87abe488-b493-4f8f-9d71-45cb1d2ddf51"
      },
      "source": [
        "# 5 fold cross validation with GRU-based model\n",
        "import torch\n",
        "X = np.array(grouped_data.keys())\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "train_losses = list()\n",
        "train_aucs = list()\n",
        "val_losses = list()\n",
        "val_aucs = list()\n",
        "train_eval = list()\n",
        "test_eval = list()\n",
        "for train, test in kfold.split(X):\n",
        "    users_train, users_test =  X[train], X[test]\n",
        "    n = len(users_test)//2\n",
        "    users_test, users_val = users_test[:n], users_test[n: ]\n",
        "    train_data_space = SPACE_DATASET(grouped_data[users_train], MAXLENGTH)\n",
        "    val_data_space = SPACE_DATASET(grouped_data[users_val], MAXLENGTH)\n",
        "    test_data_space = SPACE_DATASET(grouped_data[users_test], MAXLENGTH)\n",
        "    #construct training input\n",
        "    train_chapter=[]\n",
        "    train_sub_chapter=[]\n",
        "    train_question = []\n",
        "    train_features=[]\n",
        "    train_labels=[]\n",
        "    for i in range(len(users_train)):\n",
        "        user = train_data_space.__getitem__(i)\n",
        "        train_chapter.append(user[0])\n",
        "        train_sub_chapter.append(user[1]) \n",
        "        train_question.append(user[2])\n",
        "        train_features.append(user[3])\n",
        "        train_labels.append(user[4])\n",
        "    train_chapter = np.array(train_chapter)\n",
        "    train_sub_chapter = np.array(train_sub_chapter)\n",
        "    train_question = np.array(train_question)\n",
        "    train_features = np.array(train_features)\n",
        "    train_labels= np.array(train_labels)[..., np.newaxis]\n",
        "\n",
        "    #construct validation input\n",
        "    val_chapter=[]\n",
        "    val_sub_chapter=[]\n",
        "    val_question = []\n",
        "    val_features=[]\n",
        "    val_labels=[]\n",
        "    for i in range(len(users_val)):\n",
        "        user = val_data_space.__getitem__(i)\n",
        "        val_chapter.append(user[0])\n",
        "        val_sub_chapter.append(user[1]) \n",
        "        val_question.append(user[2])\n",
        "        val_features.append(user[3])\n",
        "        val_labels.append(user[4])\n",
        "    val_chapter = np.array(val_chapter)\n",
        "    val_sub_chapter = np.array(val_sub_chapter)\n",
        "    val_features = np.array(val_features)\n",
        "    val_question = np.array(val_question)\n",
        "    val_labels= np.array(val_labels)[..., np.newaxis]\n",
        "\n",
        "    # construct test input\n",
        "    test_chapter=[]\n",
        "    test_sub_chapter=[]\n",
        "    test_features=[]\n",
        "    test_question=[]\n",
        "    test_labels=[]\n",
        "    for i in range(len(users_test)):\n",
        "        user = test_data_space.__getitem__(i)\n",
        "        test_chapter.append(user[0])\n",
        "        test_sub_chapter.append(user[1]) \n",
        "        test_question.append(user[2])\n",
        "        test_features.append(user[3])\n",
        "        test_labels.append(user[4])\n",
        "    test_chapter = np.array(test_chapter)\n",
        "    test_sub_chapter = np.array(test_sub_chapter)\n",
        "    test_features = np.array(test_features)\n",
        "    test_question = np.array(test_question)\n",
        "    test_labels= np.array(test_labels)[..., np.newaxis]\n",
        "\n",
        "    # define loss function and evaluation metrics\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    acc = tf.keras.metrics.Accuracy()\n",
        "    auc = tf.keras.metrics.AUC()\n",
        "\n",
        "    def masked_bce(y_true, y_pred):\n",
        "      flat_pred = y_pred\n",
        "      flat_ground_truth = y_true\n",
        "      label_mask = tf.math.not_equal(flat_ground_truth, -1)\n",
        "      return bce(flat_ground_truth, flat_pred, sample_weight=label_mask)\n",
        "\n",
        "    def masked_acc(y_true, y_pred):\n",
        "      flat_pred = y_pred\n",
        "      flat_ground_truth = y_true\n",
        "      flat_pred = (flat_pred >= 0.5)\n",
        "      label_mask = tf.math.not_equal(flat_ground_truth, -1)\n",
        "      return acc(flat_ground_truth, flat_pred, sample_weight=label_mask)\n",
        "\n",
        "    def masked_auc(y_true, y_pred):\n",
        "      flat_pred = y_pred\n",
        "      flat_ground_truth = y_true\n",
        "      label_mask = tf.math.not_equal(flat_ground_truth, -1)\n",
        "      return auc(flat_ground_truth, flat_pred, sample_weight=label_mask)\n",
        "\n",
        "    # input layer\n",
        "    input_chap = tf.keras.Input(shape=(MAXLENGTH))\n",
        "    input_sub_chap = tf.keras.Input(shape=(MAXLENGTH))\n",
        "    input_ques =  tf.keras.Input(shape=(MAXLENGTH))\n",
        "    input_features = tf.keras.Input(shape=(MAXLENGTH, FEATURES_SIZE))\n",
        "\n",
        "    # embedding layer for categorical features\n",
        "    embedding_chap = Embedding(input_dim = CHAPTER_SIZE, output_dim = EMBEDDING_DIM)(input_chap)\n",
        "    embedding_sub_chap = Embedding(input_dim = SUB_CHAPTER_SIZE, output_dim = EMBEDDING_DIM)(input_sub_chap) \n",
        "    embedding_ques = Embedding(input_dim = QUESTION_SIZE, output_dim = EMBEDDING_DIM)(input_ques)       \n",
        "\n",
        "    # dense layer for numeric features\n",
        "    dense_features = Dense(EMBEDDING_DIM,input_shape = (None, MAXLENGTH))(input_features)\n",
        "\n",
        "    # definr gru layers\n",
        "    gru_chap = GRU(GRU_NEURON, input_shape = (None, EMBEDDING_DIM),return_sequences = True)(embedding_chap)\n",
        "    gru_sub_chap = GRU(GRU_NEURON, input_shape = (None, EMBEDDING_DIM),return_sequences = True)(embedding_sub_chap)\n",
        "    gru_ques = GRU(GRU_NEURON, input_shape = (None, EMBEDDING_DIM),return_sequences = True)(embedding_ques)\n",
        "    gru_features = GRU(GRU_NEURON, input_shape = (None, EMBEDDING_DIM),return_sequences = True)(dense_features)\n",
        "\n",
        "    gru_output = tf.concat([gru_chap, gru_sub_chap, gru_ques,gru_features], axis = 2)\n",
        "\n",
        "    dense1 = Dense(256, input_shape = (None, 4*EMBEDDING_DIM), activation='relu')(lstm_output)\n",
        "    dropout1 = Dropout(0.1)(dense1)\n",
        "    dense2 = Dense(64, input_shape = (None, 256), activation='relu')(dropout1)\n",
        "    dropout2 = Dropout(0.1)(dense2)\n",
        "    pred = Dense(1, input_shape = (None, 64), activation='sigmoid')(dropout2)\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs=[input_chap, input_sub_chap,input_ques, input_features],\n",
        "        outputs=pred,\n",
        "        name='gru_model'\n",
        "    )\n",
        "\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    opt_adam = Adam(learning_rate = 0.005)\n",
        "    model.compile(\n",
        "        optimizer=opt_adam,\n",
        "        loss= masked_bce,\n",
        "        metrics = [masked_acc, masked_auc]\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "      [train_chapter, train_sub_chapter, train_question, train_features],\n",
        "      train_labels,\n",
        "      batch_size = 64,\n",
        "      epochs = 100,\n",
        "      validation_data=([val_chapter, val_sub_chapter, val_question, val_features], val_labels),\n",
        "      callbacks=[callback]\n",
        "    )\n",
        "    val_losses.append(list(history.history['val_loss']))\n",
        "    train_losses.append(list(history.history['loss']))\n",
        "    val_aucs.append(list(history.history['val_masked_auc']))\n",
        "    train_aucs.append(list(history.history['masked_auc']))\n",
        "    train_score = model.evaluate([train_chapter, train_sub_chapter, train_question, train_features], train_labels)\n",
        "    train_eval.append(train_score)\n",
        "    test_score = model.evaluate([test_chapter, test_sub_chapter, test_question, test_features], test_labels)\n",
        "    test_eval.append(test_score)\n",
        "    print(\"Test: \", test_score)\n",
        "    def reset_weights(model):\n",
        "      for layer in model.layers: \n",
        "        if isinstance(layer, tf.keras.Model):\n",
        "          reset_weights(layer)\n",
        "          continue\n",
        "        for k, initializer in layer.__dict__.items():\n",
        "          if \"initializer\" not in k:\n",
        "            continue\n",
        "          # find the corresponding variable\n",
        "          var = getattr(layer, k.replace(\"_initializer\", \"\"))\n",
        "          var.assign(initializer(var.shape, var.dtype))\n",
        "    reset_weights(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 39s 392ms/step - loss: 0.6166 - masked_acc: 0.5511 - masked_auc: 0.5542 - val_loss: 0.5238 - val_masked_acc: 0.6282 - val_masked_auc: 0.6742\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.5230 - masked_acc: 0.6482 - masked_auc: 0.7019 - val_loss: 0.4940 - val_masked_acc: 0.6754 - val_masked_auc: 0.7380\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.4967 - masked_acc: 0.6828 - masked_auc: 0.7472 - val_loss: 0.4915 - val_masked_acc: 0.6944 - val_masked_auc: 0.7618\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.4904 - masked_acc: 0.6981 - masked_auc: 0.7659 - val_loss: 0.4796 - val_masked_acc: 0.7056 - val_masked_auc: 0.7744\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.4835 - masked_acc: 0.7083 - masked_auc: 0.7775 - val_loss: 0.4727 - val_masked_acc: 0.7134 - val_masked_auc: 0.7833\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.4755 - masked_acc: 0.7157 - masked_auc: 0.7858 - val_loss: 0.4671 - val_masked_acc: 0.7195 - val_masked_auc: 0.7902\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.4691 - masked_acc: 0.7212 - masked_auc: 0.7920 - val_loss: 0.4625 - val_masked_acc: 0.7244 - val_masked_auc: 0.7957\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.4617 - masked_acc: 0.7257 - masked_auc: 0.7971 - val_loss: 0.4605 - val_masked_acc: 0.7284 - val_masked_auc: 0.8002\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.4599 - masked_acc: 0.7296 - masked_auc: 0.8015 - val_loss: 0.4615 - val_masked_acc: 0.7319 - val_masked_auc: 0.8040\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.4518 - masked_acc: 0.7330 - masked_auc: 0.8052 - val_loss: 0.4582 - val_masked_acc: 0.7349 - val_masked_auc: 0.8074\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.4409 - masked_acc: 0.7361 - masked_auc: 0.8087 - val_loss: 0.4556 - val_masked_acc: 0.7378 - val_masked_auc: 0.8106\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.4378 - masked_acc: 0.7387 - masked_auc: 0.8116 - val_loss: 0.4527 - val_masked_acc: 0.7404 - val_masked_auc: 0.8136\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.4374 - masked_acc: 0.7413 - masked_auc: 0.8145 - val_loss: 0.4528 - val_masked_acc: 0.7428 - val_masked_auc: 0.8162\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 1s 98ms/step - loss: 0.4359 - masked_acc: 0.7435 - masked_auc: 0.8170 - val_loss: 0.4594 - val_masked_acc: 0.7449 - val_masked_auc: 0.8185\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.4336 - masked_acc: 0.7455 - masked_auc: 0.8192 - val_loss: 0.4523 - val_masked_acc: 0.7468 - val_masked_auc: 0.8208\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.4263 - masked_acc: 0.7475 - masked_auc: 0.8215 - val_loss: 0.4503 - val_masked_acc: 0.7487 - val_masked_auc: 0.8229\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.4245 - masked_acc: 0.7493 - masked_auc: 0.8236 - val_loss: 0.4516 - val_masked_acc: 0.7504 - val_masked_auc: 0.8248\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.4272 - masked_acc: 0.7510 - masked_auc: 0.8255 - val_loss: 0.4560 - val_masked_acc: 0.7519 - val_masked_auc: 0.8265\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 1s 97ms/step - loss: 0.4212 - masked_acc: 0.7525 - masked_auc: 0.8271 - val_loss: 0.4533 - val_masked_acc: 0.7534 - val_masked_auc: 0.8281\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.4208 - masked_acc: 0.7539 - masked_auc: 0.8287 - val_loss: 0.4632 - val_masked_acc: 0.7548 - val_masked_auc: 0.8297\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 1s 98ms/step - loss: 0.4099 - masked_acc: 0.7553 - masked_auc: 0.8302 - val_loss: 0.4573 - val_masked_acc: 0.7562 - val_masked_auc: 0.8312\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.4167 - masked_acc: 0.7566 - masked_auc: 0.8317 - val_loss: 0.4687 - val_masked_acc: 0.7574 - val_masked_auc: 0.8326\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.4067 - masked_acc: 0.7579 - masked_auc: 0.8332 - val_loss: 0.4621 - val_masked_acc: 0.7587 - val_masked_auc: 0.8341\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.4042 - masked_acc: 0.7591 - masked_auc: 0.8346 - val_loss: 0.4726 - val_masked_acc: 0.7600 - val_masked_auc: 0.8355\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.4024 - masked_acc: 0.7604 - masked_auc: 0.8360 - val_loss: 0.4739 - val_masked_acc: 0.7611 - val_masked_auc: 0.8368\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.4047 - masked_acc: 0.7615 - masked_auc: 0.8373 - val_loss: 0.4803 - val_masked_acc: 0.7622 - val_masked_auc: 0.8380\n",
            "22/22 [==============================] - 2s 36ms/step - loss: 0.3955 - masked_acc: 0.7628 - masked_auc: 0.8387\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 0.4943 - masked_acc: 0.7633 - masked_auc: 0.8393\n",
            "Test:  [0.49426954984664917, 0.763336181640625, 0.8393409848213196]\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 8s 226ms/step - loss: 0.6187 - masked_acc: 0.5563 - masked_auc: 0.5530 - val_loss: 0.5212 - val_masked_acc: 0.6299 - val_masked_auc: 0.6804\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.5165 - masked_acc: 0.6511 - masked_auc: 0.7074 - val_loss: 0.5012 - val_masked_acc: 0.6766 - val_masked_auc: 0.7405\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4936 - masked_acc: 0.6847 - masked_auc: 0.7501 - val_loss: 0.4805 - val_masked_acc: 0.6961 - val_masked_auc: 0.7635\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4841 - masked_acc: 0.7007 - masked_auc: 0.7686 - val_loss: 0.4907 - val_masked_acc: 0.7072 - val_masked_auc: 0.7764\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4812 - masked_acc: 0.7100 - masked_auc: 0.7795 - val_loss: 0.4699 - val_masked_acc: 0.7147 - val_masked_auc: 0.7848\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4793 - masked_acc: 0.7166 - masked_auc: 0.7868 - val_loss: 0.4625 - val_masked_acc: 0.7203 - val_masked_auc: 0.7910\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4735 - masked_acc: 0.7219 - masked_auc: 0.7927 - val_loss: 0.4667 - val_masked_acc: 0.7244 - val_masked_auc: 0.7957\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4656 - masked_acc: 0.7257 - masked_auc: 0.7971 - val_loss: 0.4569 - val_masked_acc: 0.7280 - val_masked_auc: 0.7997\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4661 - masked_acc: 0.7291 - masked_auc: 0.8009 - val_loss: 0.4850 - val_masked_acc: 0.7309 - val_masked_auc: 0.8029\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.4766 - masked_acc: 0.7313 - masked_auc: 0.8033 - val_loss: 0.4634 - val_masked_acc: 0.7326 - val_masked_auc: 0.8049\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4673 - masked_acc: 0.7333 - masked_auc: 0.8056 - val_loss: 0.4578 - val_masked_acc: 0.7346 - val_masked_auc: 0.8070\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4653 - masked_acc: 0.7352 - masked_auc: 0.8076 - val_loss: 0.4650 - val_masked_acc: 0.7365 - val_masked_auc: 0.8090\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4578 - masked_acc: 0.7371 - masked_auc: 0.8096 - val_loss: 0.4631 - val_masked_acc: 0.7382 - val_masked_auc: 0.8108\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4527 - masked_acc: 0.7388 - masked_auc: 0.8115 - val_loss: 0.4558 - val_masked_acc: 0.7398 - val_masked_auc: 0.8127\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4464 - masked_acc: 0.7405 - masked_auc: 0.8134 - val_loss: 0.4565 - val_masked_acc: 0.7416 - val_masked_auc: 0.8147\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4430 - masked_acc: 0.7422 - masked_auc: 0.8153 - val_loss: 0.4587 - val_masked_acc: 0.7432 - val_masked_auc: 0.8164\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4511 - masked_acc: 0.7436 - masked_auc: 0.8168 - val_loss: 0.4868 - val_masked_acc: 0.7445 - val_masked_auc: 0.8177\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4711 - masked_acc: 0.7447 - masked_auc: 0.8178 - val_loss: 0.4802 - val_masked_acc: 0.7450 - val_masked_auc: 0.8181\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4642 - masked_acc: 0.7452 - masked_auc: 0.8183 - val_loss: 0.4580 - val_masked_acc: 0.7456 - val_masked_auc: 0.8188\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4538 - masked_acc: 0.7459 - masked_auc: 0.8190 - val_loss: 0.4511 - val_masked_acc: 0.7465 - val_masked_auc: 0.8197\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4491 - masked_acc: 0.7468 - masked_auc: 0.8200 - val_loss: 0.4564 - val_masked_acc: 0.7473 - val_masked_auc: 0.8206\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4555 - masked_acc: 0.7475 - masked_auc: 0.8208 - val_loss: 0.4519 - val_masked_acc: 0.7479 - val_masked_auc: 0.8213\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4552 - masked_acc: 0.7482 - masked_auc: 0.8215 - val_loss: 0.4541 - val_masked_acc: 0.7486 - val_masked_auc: 0.8219\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4480 - masked_acc: 0.7489 - masked_auc: 0.8222 - val_loss: 0.4569 - val_masked_acc: 0.7494 - val_masked_auc: 0.8228\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4470 - masked_acc: 0.7497 - masked_auc: 0.8231 - val_loss: 0.4590 - val_masked_acc: 0.7502 - val_masked_auc: 0.8237\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.4454 - masked_acc: 0.7504 - masked_auc: 0.8239 - val_loss: 0.4617 - val_masked_acc: 0.7508 - val_masked_auc: 0.8244\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4413 - masked_acc: 0.7511 - masked_auc: 0.8246 - val_loss: 0.4574 - val_masked_acc: 0.7516 - val_masked_auc: 0.8252\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4392 - masked_acc: 0.7519 - masked_auc: 0.8255 - val_loss: 0.4611 - val_masked_acc: 0.7523 - val_masked_auc: 0.8260\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4461 - masked_acc: 0.7525 - masked_auc: 0.8262 - val_loss: 0.4583 - val_masked_acc: 0.7529 - val_masked_auc: 0.8267\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4312 - masked_acc: 0.7531 - masked_auc: 0.8269 - val_loss: 0.4560 - val_masked_acc: 0.7536 - val_masked_auc: 0.8275\n",
            "23/23 [==============================] - 1s 38ms/step - loss: 0.4284 - masked_acc: 0.7539 - masked_auc: 0.8279\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.4831 - masked_acc: 0.7543 - masked_auc: 0.8283\n",
            "Test:  [0.4830905795097351, 0.7542741894721985, 0.8282527327537537]\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 8s 223ms/step - loss: 0.6316 - masked_acc: 0.5213 - masked_auc: 0.5403 - val_loss: 0.5352 - val_masked_acc: 0.6129 - val_masked_auc: 0.6560\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.5280 - masked_acc: 0.6355 - masked_auc: 0.6862 - val_loss: 0.5043 - val_masked_acc: 0.6654 - val_masked_auc: 0.7262\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4930 - masked_acc: 0.6745 - masked_auc: 0.7377 - val_loss: 0.4912 - val_masked_acc: 0.6867 - val_masked_auc: 0.7532\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4826 - masked_acc: 0.6916 - masked_auc: 0.7590 - val_loss: 0.5036 - val_masked_acc: 0.6989 - val_masked_auc: 0.7673\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.5144 - masked_acc: 0.7005 - masked_auc: 0.7687 - val_loss: 0.4981 - val_masked_acc: 0.7037 - val_masked_auc: 0.7721\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4898 - masked_acc: 0.7056 - masked_auc: 0.7742 - val_loss: 0.4863 - val_masked_acc: 0.7088 - val_masked_auc: 0.7779\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4828 - masked_acc: 0.7104 - masked_auc: 0.7798 - val_loss: 0.4769 - val_masked_acc: 0.7135 - val_masked_auc: 0.7833\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4782 - masked_acc: 0.7149 - masked_auc: 0.7849 - val_loss: 0.4720 - val_masked_acc: 0.7175 - val_masked_auc: 0.7878\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4672 - masked_acc: 0.7187 - masked_auc: 0.7891 - val_loss: 0.4809 - val_masked_acc: 0.7210 - val_masked_auc: 0.7918\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4774 - masked_acc: 0.7220 - masked_auc: 0.7927 - val_loss: 0.4698 - val_masked_acc: 0.7237 - val_masked_auc: 0.7946\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.4708 - masked_acc: 0.7246 - masked_auc: 0.7956 - val_loss: 0.4654 - val_masked_acc: 0.7260 - val_masked_auc: 0.7971\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4654 - masked_acc: 0.7268 - masked_auc: 0.7979 - val_loss: 0.4708 - val_masked_acc: 0.7281 - val_masked_auc: 0.7995\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4644 - masked_acc: 0.7288 - masked_auc: 0.8001 - val_loss: 0.4693 - val_masked_acc: 0.7302 - val_masked_auc: 0.8018\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4526 - masked_acc: 0.7310 - masked_auc: 0.8026 - val_loss: 0.4750 - val_masked_acc: 0.7322 - val_masked_auc: 0.8040\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4610 - masked_acc: 0.7328 - masked_auc: 0.8047 - val_loss: 0.4663 - val_masked_acc: 0.7340 - val_masked_auc: 0.8060\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4669 - masked_acc: 0.7343 - masked_auc: 0.8063 - val_loss: 0.4799 - val_masked_acc: 0.7349 - val_masked_auc: 0.8068\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4768 - masked_acc: 0.7352 - masked_auc: 0.8071 - val_loss: 0.4732 - val_masked_acc: 0.7359 - val_masked_auc: 0.8078\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4622 - masked_acc: 0.7362 - masked_auc: 0.8081 - val_loss: 0.4627 - val_masked_acc: 0.7370 - val_masked_auc: 0.8090\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4533 - masked_acc: 0.7375 - masked_auc: 0.8095 - val_loss: 0.4649 - val_masked_acc: 0.7383 - val_masked_auc: 0.8104\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.4508 - masked_acc: 0.7388 - masked_auc: 0.8109 - val_loss: 0.4602 - val_masked_acc: 0.7397 - val_masked_auc: 0.8119\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4446 - masked_acc: 0.7401 - masked_auc: 0.8124 - val_loss: 0.4681 - val_masked_acc: 0.7410 - val_masked_auc: 0.8132\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4572 - masked_acc: 0.7413 - masked_auc: 0.8135 - val_loss: 0.4618 - val_masked_acc: 0.7421 - val_masked_auc: 0.8144\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4434 - masked_acc: 0.7425 - masked_auc: 0.8148 - val_loss: 0.4591 - val_masked_acc: 0.7432 - val_masked_auc: 0.8156\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4433 - masked_acc: 0.7435 - masked_auc: 0.8159 - val_loss: 0.4556 - val_masked_acc: 0.7442 - val_masked_auc: 0.8168\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4375 - masked_acc: 0.7446 - masked_auc: 0.8172 - val_loss: 0.4580 - val_masked_acc: 0.7452 - val_masked_auc: 0.8178\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4390 - masked_acc: 0.7456 - masked_auc: 0.8182 - val_loss: 0.4730 - val_masked_acc: 0.7461 - val_masked_auc: 0.8187\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 1s 96ms/step - loss: 0.4590 - masked_acc: 0.7463 - masked_auc: 0.8189 - val_loss: 0.4698 - val_masked_acc: 0.7467 - val_masked_auc: 0.8192\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4488 - masked_acc: 0.7469 - masked_auc: 0.8194 - val_loss: 0.4693 - val_masked_acc: 0.7473 - val_masked_auc: 0.8199\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4405 - masked_acc: 0.7476 - masked_auc: 0.8202 - val_loss: 0.4668 - val_masked_acc: 0.7481 - val_masked_auc: 0.8208\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4384 - masked_acc: 0.7484 - masked_auc: 0.8211 - val_loss: 0.4671 - val_masked_acc: 0.7488 - val_masked_auc: 0.8216\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4401 - masked_acc: 0.7490 - masked_auc: 0.8218 - val_loss: 0.4663 - val_masked_acc: 0.7495 - val_masked_auc: 0.8224\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4298 - masked_acc: 0.7498 - masked_auc: 0.8227 - val_loss: 0.4588 - val_masked_acc: 0.7503 - val_masked_auc: 0.8233\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4434 - masked_acc: 0.7505 - masked_auc: 0.8235 - val_loss: 0.4588 - val_masked_acc: 0.7508 - val_masked_auc: 0.8239\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4363 - masked_acc: 0.7510 - masked_auc: 0.8242 - val_loss: 0.4578 - val_masked_acc: 0.7514 - val_masked_auc: 0.8247\n",
            "23/23 [==============================] - 1s 38ms/step - loss: 0.4251 - masked_acc: 0.7517 - masked_auc: 0.8251\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 0.4954 - masked_acc: 0.7521 - masked_auc: 0.8255\n",
            "Test:  [0.4954483211040497, 0.7520955204963684, 0.8254714012145996]\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 8s 250ms/step - loss: 0.6176 - masked_acc: 0.5600 - masked_auc: 0.5667 - val_loss: 0.5208 - val_masked_acc: 0.6343 - val_masked_auc: 0.6819\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.5158 - masked_acc: 0.6527 - masked_auc: 0.7080 - val_loss: 0.4917 - val_masked_acc: 0.6763 - val_masked_auc: 0.7393\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4993 - masked_acc: 0.6839 - masked_auc: 0.7487 - val_loss: 0.4921 - val_masked_acc: 0.6940 - val_masked_auc: 0.7612\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4941 - masked_acc: 0.6978 - masked_auc: 0.7659 - val_loss: 0.4800 - val_masked_acc: 0.7047 - val_masked_auc: 0.7739\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4811 - masked_acc: 0.7077 - masked_auc: 0.7774 - val_loss: 0.4747 - val_masked_acc: 0.7119 - val_masked_auc: 0.7822\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4777 - masked_acc: 0.7139 - masked_auc: 0.7844 - val_loss: 0.4758 - val_masked_acc: 0.7172 - val_masked_auc: 0.7880\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4825 - masked_acc: 0.7187 - masked_auc: 0.7894 - val_loss: 0.4693 - val_masked_acc: 0.7209 - val_masked_auc: 0.7915\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4881 - masked_acc: 0.7220 - masked_auc: 0.7925 - val_loss: 0.4740 - val_masked_acc: 0.7238 - val_masked_auc: 0.7944\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4821 - masked_acc: 0.7247 - masked_auc: 0.7954 - val_loss: 0.4612 - val_masked_acc: 0.7266 - val_masked_auc: 0.7975\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 1s 97ms/step - loss: 0.4711 - masked_acc: 0.7276 - masked_auc: 0.7984 - val_loss: 0.4811 - val_masked_acc: 0.7293 - val_masked_auc: 0.7999\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4752 - masked_acc: 0.7299 - masked_auc: 0.8006 - val_loss: 0.4599 - val_masked_acc: 0.7307 - val_masked_auc: 0.8015\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4708 - masked_acc: 0.7313 - masked_auc: 0.8022 - val_loss: 0.4665 - val_masked_acc: 0.7322 - val_masked_auc: 0.8033\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4668 - masked_acc: 0.7329 - masked_auc: 0.8040 - val_loss: 0.4583 - val_masked_acc: 0.7339 - val_masked_auc: 0.8052\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4605 - masked_acc: 0.7345 - masked_auc: 0.8058 - val_loss: 0.4581 - val_masked_acc: 0.7356 - val_masked_auc: 0.8071\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4548 - masked_acc: 0.7362 - masked_auc: 0.8077 - val_loss: 0.4579 - val_masked_acc: 0.7372 - val_masked_auc: 0.8089\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4650 - masked_acc: 0.7376 - masked_auc: 0.8092 - val_loss: 0.4510 - val_masked_acc: 0.7386 - val_masked_auc: 0.8104\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4545 - masked_acc: 0.7392 - masked_auc: 0.8110 - val_loss: 0.4541 - val_masked_acc: 0.7400 - val_masked_auc: 0.8119\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 105ms/step - loss: 0.4575 - masked_acc: 0.7404 - masked_auc: 0.8124 - val_loss: 0.4650 - val_masked_acc: 0.7411 - val_masked_auc: 0.8131\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4524 - masked_acc: 0.7415 - masked_auc: 0.8136 - val_loss: 0.4540 - val_masked_acc: 0.7422 - val_masked_auc: 0.8144\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4524 - masked_acc: 0.7426 - masked_auc: 0.8148 - val_loss: 0.4550 - val_masked_acc: 0.7433 - val_masked_auc: 0.8156\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4447 - masked_acc: 0.7437 - masked_auc: 0.8160 - val_loss: 0.4573 - val_masked_acc: 0.7444 - val_masked_auc: 0.8168\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4473 - masked_acc: 0.7447 - masked_auc: 0.8172 - val_loss: 0.4559 - val_masked_acc: 0.7452 - val_masked_auc: 0.8177\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4502 - masked_acc: 0.7456 - masked_auc: 0.8181 - val_loss: 0.4993 - val_masked_acc: 0.7460 - val_masked_auc: 0.8186\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4978 - masked_acc: 0.7458 - masked_auc: 0.8183 - val_loss: 0.4806 - val_masked_acc: 0.7458 - val_masked_auc: 0.8183\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4718 - masked_acc: 0.7459 - masked_auc: 0.8184 - val_loss: 0.4655 - val_masked_acc: 0.7462 - val_masked_auc: 0.8185\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4703 - masked_acc: 0.7463 - masked_auc: 0.8186 - val_loss: 0.4565 - val_masked_acc: 0.7465 - val_masked_auc: 0.8189\n",
            "23/23 [==============================] - 1s 39ms/step - loss: 0.4581 - masked_acc: 0.7467 - masked_auc: 0.8191\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.4995 - masked_acc: 0.7470 - masked_auc: 0.8193\n",
            "Test:  [0.4995304346084595, 0.7469682693481445, 0.8193071484565735]\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 8s 225ms/step - loss: 0.6219 - masked_acc: 0.5279 - masked_auc: 0.5406 - val_loss: 0.5520 - val_masked_acc: 0.6244 - val_masked_auc: 0.6690\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.5372 - masked_acc: 0.6426 - masked_auc: 0.6948 - val_loss: 0.4948 - val_masked_acc: 0.6679 - val_masked_auc: 0.7280\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.5053 - masked_acc: 0.6758 - masked_auc: 0.7381 - val_loss: 0.4879 - val_masked_acc: 0.6885 - val_masked_auc: 0.7540\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4909 - masked_acc: 0.6930 - masked_auc: 0.7595 - val_loss: 0.5008 - val_masked_acc: 0.7000 - val_masked_auc: 0.7674\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4979 - masked_acc: 0.7026 - masked_auc: 0.7703 - val_loss: 0.4754 - val_masked_acc: 0.7069 - val_masked_auc: 0.7749\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.4921 - masked_acc: 0.7089 - masked_auc: 0.7770 - val_loss: 0.4732 - val_masked_acc: 0.7120 - val_masked_auc: 0.7807\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4861 - masked_acc: 0.7136 - masked_auc: 0.7826 - val_loss: 0.4680 - val_masked_acc: 0.7165 - val_masked_auc: 0.7858\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4755 - masked_acc: 0.7181 - masked_auc: 0.7875 - val_loss: 0.4650 - val_masked_acc: 0.7206 - val_masked_auc: 0.7903\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4698 - masked_acc: 0.7219 - masked_auc: 0.7918 - val_loss: 0.4728 - val_masked_acc: 0.7239 - val_masked_auc: 0.7940\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4702 - masked_acc: 0.7249 - masked_auc: 0.7953 - val_loss: 0.4589 - val_masked_acc: 0.7266 - val_masked_auc: 0.7970\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.4675 - masked_acc: 0.7274 - masked_auc: 0.7979 - val_loss: 0.4655 - val_masked_acc: 0.7287 - val_masked_auc: 0.7994\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4784 - masked_acc: 0.7292 - masked_auc: 0.7998 - val_loss: 0.4764 - val_masked_acc: 0.7302 - val_masked_auc: 0.8009\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4699 - masked_acc: 0.7308 - masked_auc: 0.8015 - val_loss: 0.4615 - val_masked_acc: 0.7319 - val_masked_auc: 0.8028\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4621 - masked_acc: 0.7326 - masked_auc: 0.8035 - val_loss: 0.4566 - val_masked_acc: 0.7337 - val_masked_auc: 0.8048\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4512 - masked_acc: 0.7344 - masked_auc: 0.8056 - val_loss: 0.4558 - val_masked_acc: 0.7355 - val_masked_auc: 0.8068\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4572 - masked_acc: 0.7361 - masked_auc: 0.8074 - val_loss: 0.4567 - val_masked_acc: 0.7372 - val_masked_auc: 0.8087\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4488 - masked_acc: 0.7378 - masked_auc: 0.8093 - val_loss: 0.4584 - val_masked_acc: 0.7387 - val_masked_auc: 0.8104\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 98ms/step - loss: 0.4537 - masked_acc: 0.7391 - masked_auc: 0.8109 - val_loss: 0.4749 - val_masked_acc: 0.7400 - val_masked_auc: 0.8118\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4554 - masked_acc: 0.7404 - masked_auc: 0.8122 - val_loss: 0.4531 - val_masked_acc: 0.7412 - val_masked_auc: 0.8130\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4461 - masked_acc: 0.7417 - masked_auc: 0.8136 - val_loss: 0.4572 - val_masked_acc: 0.7425 - val_masked_auc: 0.8145\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4527 - masked_acc: 0.7428 - masked_auc: 0.8149 - val_loss: 0.4629 - val_masked_acc: 0.7435 - val_masked_auc: 0.8157\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4435 - masked_acc: 0.7439 - masked_auc: 0.8162 - val_loss: 0.4532 - val_masked_acc: 0.7446 - val_masked_auc: 0.8169\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4458 - masked_acc: 0.7449 - masked_auc: 0.8173 - val_loss: 0.4535 - val_masked_acc: 0.7455 - val_masked_auc: 0.8180\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.4430 - masked_acc: 0.7459 - masked_auc: 0.8185 - val_loss: 0.4600 - val_masked_acc: 0.7465 - val_masked_auc: 0.8192\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4394 - masked_acc: 0.7469 - masked_auc: 0.8196 - val_loss: 0.4598 - val_masked_acc: 0.7475 - val_masked_auc: 0.8203\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4322 - masked_acc: 0.7478 - masked_auc: 0.8207 - val_loss: 0.4741 - val_masked_acc: 0.7483 - val_masked_auc: 0.8213\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 1s 101ms/step - loss: 0.4583 - masked_acc: 0.7485 - masked_auc: 0.8214 - val_loss: 0.4558 - val_masked_acc: 0.7487 - val_masked_auc: 0.8216\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.4635 - masked_acc: 0.7488 - masked_auc: 0.8218 - val_loss: 0.4825 - val_masked_acc: 0.7492 - val_masked_auc: 0.8221\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.4678 - masked_acc: 0.7494 - masked_auc: 0.8222 - val_loss: 0.4695 - val_masked_acc: 0.7496 - val_masked_auc: 0.8224\n",
            "23/23 [==============================] - 1s 39ms/step - loss: 0.4452 - masked_acc: 0.7499 - masked_auc: 0.8227\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.4860 - masked_acc: 0.7502 - masked_auc: 0.8230\n",
            "Test:  [0.4859600067138672, 0.7502043843269348, 0.823045551776886]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVmumHMz3lx",
        "outputId": "4ff1e2fa-6abb-458e-c729-495b456f53e5"
      },
      "source": [
        "t_eval = np.array(test_eval)\n",
        "print(\"test avg loss: \", np.mean(t_eval[:, 0]), \"+/-\" ,np.std(t_eval[:, 0]))\n",
        "print(\"test avg acc: \", np.mean(t_eval[:, 1]),  \"+/-\" ,np.std(t_eval[:, 1]))\n",
        "print(\"test avg auc: \", np.mean(t_eval[:, 2]), \"+/-\" ,np.std(t_eval[:, 2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test avg loss:  0.4916597783565521 +/- 0.006148654468595115\n",
            "test avg acc:  0.7533757090568542 +/- 0.005527562719002895\n",
            "test avg auc:  0.8270835638046264 +/- 0.006796890039280701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9MM_CXWz5K6",
        "outputId": "4cf88e1d-3a74-4e7d-f92c-d01522e91757"
      },
      "source": [
        "t_eval = np.array(train_eval)\n",
        "print(\"train avg loss: \", np.mean(t_eval[:, 0]), \"+/-\" ,np.std(t_eval[:, 0]))\n",
        "print(\"train avg acc: \", np.mean(t_eval[:, 1]),  \"+/-\" ,np.std(t_eval[:, 1]))\n",
        "print(\"train avg auc: \", np.mean(t_eval[:, 2]), \"+/-\" ,np.std(t_eval[:, 2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train avg loss:  0.4304681599140167 +/- 0.021162527398007086\n",
            "train avg acc:  0.7530073523521423 +/- 0.005419863564952259\n",
            "train avg auc:  0.8266833186149597 +/- 0.006665333059819493\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}